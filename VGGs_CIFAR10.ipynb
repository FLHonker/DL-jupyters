{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T02:56:18.360520Z",
     "start_time": "2019-04-21T02:56:18.353618Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# settings\n",
    "cfg = {\n",
    "    'VGG11':[64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13':[64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16':[64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19':[64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "# model\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name, num_classes=10):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for h in cfg:\n",
    "            if h == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else :\n",
    "                layers += [nn.Conv2d(in_channels, h, kernel_size=3, padding=1),\n",
    "                            nn.BatchNorm2d(h),\n",
    "                            nn.ReLU(inplace=True)]\n",
    "                in_channels = h\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers) \n",
    "\n",
    "# VGGs\n",
    "def VGG11():\n",
    "    return VGG('VGG11')\n",
    "\n",
    "def VGG13():\n",
    "    return VGG('VGG13')\n",
    "\n",
    "def VGG16():\n",
    "    return VGG('VGG16')\n",
    "\n",
    "def VGG19():\n",
    "    return VGG('VGG19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T03:49:45.985865Z",
     "start_time": "2019-04-21T03:49:45.970041Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import argparse\n",
    "\n",
    "# main\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='cifar-10 with PyTorch')\n",
    "    parser.add_argument('--lr', default=0.001, type=float, help='learning rate')\n",
    "    parser.add_argument('--epoch', default=50, type=int, help='number of epoch tp train for') \n",
    "    parser.add_argument('--trainBatchSize', default=128, type=int, help='training batch size')\n",
    "    parser.add_argument('--testBatchSize', default=128, type=int, help='testing batch size')\n",
    "    parser.add_argument('--cuda', default=torch.cuda.is_available(), type=bool, help='use cuda or not')\n",
    "    \n",
    "    config_list = ['--lr', '0.001', '--epoch', '50', '--trainBatchSize', '128', '--testBatchSize', '128', '--cuda', 'True']\n",
    "    args = parser.parse_args(config_list) \n",
    "    \n",
    "    solver = Solver(args)\n",
    "    solver.run()\n",
    "\n",
    "CLASSES = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Solver\n",
    "class Solver(object):\n",
    "    def __init__(self, config):\n",
    "        self.model = None\n",
    "        self.lr = config.lr\n",
    "        self.epochs = config.epoch\n",
    "        self.train_batch_size = config.trainBatchSize\n",
    "        self.test_batch_size = config.testBatchSize\n",
    "        self.criterion = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.device = 'cuda' if config.cuda else 'cpu'\n",
    "        self.train_loader = None\n",
    "        self.test_loader = None\n",
    "        \n",
    "    def print_model(self):\n",
    "        print(self.model)\n",
    "        \n",
    "    def load_data(self):\n",
    "        train_transform = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.ToTensor()])\n",
    "        test_transform = transforms.Compose([transforms.ToTensor()])\n",
    "        train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "        test_set = datasets.CIFAR10(root='./data', train=False, download=False, transform=test_transform)\n",
    "        self.train_loader = DataLoader(train_set, batch_size=self.train_batch_size, shuffle=True)\n",
    "        self.test_loader = DataLoader(test_set, batch_size=self.test_batch_size, shuffle=False)\n",
    "    \n",
    "    def load_model(self):\n",
    "        # self.model = LeNet().to(self.device)\n",
    "        # self.model = AlexNet().to(self.device)\n",
    "        self.model = VGG11().to(self.device)\n",
    "        # self.model = VGG13().to(self.device)\n",
    "        # self.model = VGG16().to(self.device)\n",
    "        # self.model = VGG19().to(self.device)\n",
    "        # self.model = GoogLeNet().to(self.device)\n",
    "        # self.model = resnet18().to(self.device)\n",
    "        # self.model = resnet34().to(self.device)\n",
    "        # self.model = resnet50().to(self.device)\n",
    "        # self.model = resnet101().to(self.device)\n",
    "        # self.model = resnet152().to(self.device)\n",
    "        # self.model = DenseNet121().to(self.device)\n",
    "        # self.model = DenseNet161().to(self.device)\n",
    "        # self.model = DenseNet169().to(self.device)\n",
    "        # self.model = DenseNet201().to(self.device)\n",
    "        # self.model = WideResNet(depth=28, num_classes=10).to(self.device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=[75, 150], gamma=0.5)\n",
    "        self.criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "    \n",
    "    # train\n",
    "    def train(self):\n",
    "        print('Training:')\n",
    "        self.model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0 \n",
    "        total = 0 \n",
    "        \n",
    "        for ibatch, (images, labels) in enumerate(self.train_loader):\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "            outputs = self.model(images)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, pred = torch.max(outputs, 1) # second param \"1\" represents the dimension to be reduced\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            # train_correct incremented by one if predicted right\n",
    "            # train_correct += np.sum(prediction[1].cpu().numpy() == target.cpu().numpy())\n",
    "            train_correct += (pred == labels).sum().item()\n",
    "            if ibatch % 99 == 0:\n",
    "                print('\\t{}/{}: loss = {:.4f}, Acc = {:.3f}%'.format(ibatch, len(self.train_loader), train_loss/(ibatch+1), 100. * train_correct/total))\n",
    "        return train_loss, float(train_correct/total)\n",
    "    \n",
    "    # test\n",
    "    def test(self):\n",
    "        print('Testing:')\n",
    "        self.model.eval()\n",
    "        test_loss = 0.0 \n",
    "        test_correct = 0 \n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for ibatch, (images, labels) in enumerate(self.test_loader):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                _, pred = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                test_correct += (pred == labels).sum().item()\n",
    "                if ibatch % 99 == 0:\n",
    "                    print('\\t{}/{}: loss = {:.4f}, Acc = {:.3f}%'.format(ibatch, len(self.test_loader), test_loss/(ibatch+1), 100. * test_correct/total))\n",
    "        return test_loss, float(test_correct/total)\n",
    "    \n",
    "    def save_model(self):\n",
    "        model_out_path = './model/vgg_cifar10.pth'\n",
    "        torch.save(self.model, model_out_path)\n",
    "        print(\"* Checkpoint saved to {}\".format(model_out_path))\n",
    "        \n",
    "    # run\n",
    "    def run(self):\n",
    "        self.load_data()\n",
    "        self.load_model() \n",
    "        accuracy = 0.\n",
    "        \n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.scheduler.step(epoch)\n",
    "            print(\"\\n===> epoch: {}/{}\".format(epoch, self.epochs))\n",
    "            train_result = self.train()\n",
    "            print(train_result)\n",
    "            test_result = self.test()\n",
    "            accuracy = max(accuracy, test_result[1])\n",
    "        print(\"===> BEST ACC. PERFORMANCE: {:.3f}%\".format(accuracy * 100))\n",
    "        self.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T03:33:43.711577Z",
     "start_time": "2019-04-21T03:12:06.413743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "\n",
      "===> epoch: 1/50\n",
      "Training:\n",
      "\t0/391: loss = 2.4044, Acc = 7.812%\n",
      "\t99/391: loss = 1.6178, Acc = 38.789%\n",
      "\t198/391: loss = 1.4471, Acc = 45.705%\n",
      "\t297/391: loss = 1.3331, Acc = 50.503%\n",
      "(489.9504368901253, 0.5383)\n",
      "Testing:\n",
      "\t0/79: loss = 1.0071, Acc = 68.750%\n",
      "\n",
      "===> epoch: 2/50\n",
      "Training:\n",
      "\t0/391: loss = 1.0513, Acc = 57.812%\n",
      "\t99/391: loss = 0.8568, Acc = 69.031%\n",
      "\t198/391: loss = 0.8368, Acc = 69.892%\n",
      "\t297/391: loss = 0.8115, Acc = 71.112%\n",
      "(310.74872571229935, 0.7187)\n",
      "Testing:\n",
      "\t0/79: loss = 1.0914, Acc = 64.062%\n",
      "\n",
      "===> epoch: 3/50\n",
      "Training:\n",
      "\t0/391: loss = 0.6961, Acc = 78.906%\n",
      "\t99/391: loss = 0.6594, Acc = 76.945%\n",
      "\t198/391: loss = 0.6444, Acc = 77.371%\n",
      "\t297/391: loss = 0.6370, Acc = 77.666%\n",
      "(245.2729464173317, 0.7803)\n",
      "Testing:\n",
      "\t0/79: loss = 0.6603, Acc = 75.000%\n",
      "\n",
      "===> epoch: 4/50\n",
      "Training:\n",
      "\t0/391: loss = 0.6494, Acc = 76.562%\n",
      "\t99/391: loss = 0.5344, Acc = 81.602%\n",
      "\t198/391: loss = 0.5302, Acc = 81.729%\n",
      "\t297/391: loss = 0.5250, Acc = 81.858%\n",
      "(205.69178184866905, 0.81866)\n",
      "Testing:\n",
      "\t0/79: loss = 0.5886, Acc = 79.688%\n",
      "\n",
      "===> epoch: 5/50\n",
      "Training:\n",
      "\t0/391: loss = 0.3837, Acc = 86.719%\n",
      "\t99/391: loss = 0.4435, Acc = 84.516%\n",
      "\t198/391: loss = 0.4503, Acc = 84.383%\n",
      "\t297/391: loss = 0.4571, Acc = 84.220%\n",
      "(177.53953552246094, 0.8438)\n",
      "Testing:\n",
      "\t0/79: loss = 0.6406, Acc = 78.906%\n",
      "\n",
      "===> epoch: 6/50\n",
      "Training:\n",
      "\t0/391: loss = 0.3420, Acc = 88.281%\n",
      "\t99/391: loss = 0.3762, Acc = 86.812%\n",
      "\t198/391: loss = 0.3809, Acc = 86.884%\n",
      "\t297/391: loss = 0.3805, Acc = 86.897%\n",
      "(150.69769679009914, 0.86752)\n",
      "Testing:\n",
      "\t0/79: loss = 0.4601, Acc = 83.594%\n",
      "\n",
      "===> epoch: 7/50\n",
      "Training:\n",
      "\t0/391: loss = 0.2485, Acc = 92.188%\n",
      "\t99/391: loss = 0.3056, Acc = 89.148%\n",
      "\t198/391: loss = 0.3212, Acc = 88.843%\n",
      "\t297/391: loss = 0.3244, Acc = 88.753%\n",
      "(128.989658549428, 0.88628)\n",
      "Testing:\n",
      "\t0/79: loss = 0.4958, Acc = 78.906%\n",
      "\n",
      "===> epoch: 8/50\n",
      "Training:\n",
      "\t0/391: loss = 0.4091, Acc = 86.719%\n",
      "\t99/391: loss = 0.2734, Acc = 90.477%\n",
      "\t198/391: loss = 0.2805, Acc = 90.283%\n",
      "\t297/391: loss = 0.2801, Acc = 90.344%\n",
      "(110.9102881103754, 0.90226)\n",
      "Testing:\n",
      "\t0/79: loss = 0.4777, Acc = 83.594%\n",
      "\n",
      "===> epoch: 9/50\n",
      "Training:\n",
      "\t0/391: loss = 0.2666, Acc = 92.188%\n",
      "\t99/391: loss = 0.2282, Acc = 91.961%\n",
      "\t198/391: loss = 0.2374, Acc = 91.709%\n",
      "\t297/391: loss = 0.2401, Acc = 91.697%\n",
      "(95.36567894369364, 0.91558)\n",
      "Testing:\n",
      "\t0/79: loss = 0.4540, Acc = 88.281%\n",
      "\n",
      "===> epoch: 10/50\n",
      "Training:\n",
      "\t0/391: loss = 0.2904, Acc = 92.188%\n",
      "\t99/391: loss = 0.1833, Acc = 93.625%\n",
      "\t198/391: loss = 0.1917, Acc = 93.306%\n",
      "\t297/391: loss = 0.1962, Acc = 93.199%\n",
      "(79.13157837092876, 0.92982)\n",
      "Testing:\n",
      "\t0/79: loss = 0.3885, Acc = 89.844%\n",
      "\n",
      "===> epoch: 11/50\n",
      "Training:\n",
      "\t0/391: loss = 0.1275, Acc = 96.094%\n",
      "\t99/391: loss = 0.1571, Acc = 94.688%\n",
      "\t198/391: loss = 0.1605, Acc = 94.563%\n",
      "\t297/391: loss = 0.1720, Acc = 94.151%\n",
      "(67.32198371738195, 0.94108)\n",
      "Testing:\n",
      "\t0/79: loss = 0.3647, Acc = 89.844%\n",
      "\n",
      "===> epoch: 12/50\n",
      "Training:\n",
      "\t0/391: loss = 0.1046, Acc = 96.094%\n",
      "\t99/391: loss = 0.1417, Acc = 95.141%\n",
      "\t198/391: loss = 0.1459, Acc = 95.089%\n",
      "\t297/391: loss = 0.1482, Acc = 94.977%\n",
      "(59.1992797665298, 0.94832)\n",
      "Testing:\n",
      "\t0/79: loss = 0.6250, Acc = 82.031%\n",
      "\n",
      "===> epoch: 13/50\n",
      "Training:\n",
      "\t0/391: loss = 0.1703, Acc = 95.312%\n",
      "\t99/391: loss = 0.1112, Acc = 96.180%\n",
      "\t198/391: loss = 0.1210, Acc = 95.984%\n",
      "\t297/391: loss = 0.1235, Acc = 95.821%\n",
      "(50.204681012779474, 0.95636)\n",
      "Testing:\n",
      "\t0/79: loss = 0.6085, Acc = 82.031%\n",
      "\n",
      "===> epoch: 14/50\n",
      "Training:\n",
      "\t0/391: loss = 0.3143, Acc = 91.406%\n",
      "\t99/391: loss = 0.1061, Acc = 96.383%\n",
      "\t198/391: loss = 0.1040, Acc = 96.384%\n",
      "\t297/391: loss = 0.1085, Acc = 96.238%\n",
      "(43.665504563599825, 0.96134)\n",
      "Testing:\n",
      "\t0/79: loss = 0.6595, Acc = 83.594%\n",
      "\n",
      "===> epoch: 15/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0723, Acc = 96.875%\n",
      "\t99/391: loss = 0.0906, Acc = 96.844%\n",
      "\t198/391: loss = 0.0904, Acc = 96.859%\n",
      "\t297/391: loss = 0.0949, Acc = 96.705%\n",
      "(38.16134351119399, 0.96598)\n",
      "Testing:\n",
      "\t0/79: loss = 0.4124, Acc = 90.625%\n",
      "\n",
      "===> epoch: 16/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0738, Acc = 97.656%\n",
      "\t99/391: loss = 0.0740, Acc = 97.609%\n",
      "\t198/391: loss = 0.0816, Acc = 97.338%\n",
      "\t297/391: loss = 0.0866, Acc = 97.114%\n",
      "(34.445124009624124, 0.9703)\n",
      "Testing:\n",
      "\t0/79: loss = 0.5467, Acc = 87.500%\n",
      "\n",
      "===> epoch: 17/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0439, Acc = 99.219%\n",
      "\t99/391: loss = 0.0774, Acc = 97.281%\n",
      "\t198/391: loss = 0.0758, Acc = 97.385%\n",
      "\t297/391: loss = 0.0763, Acc = 97.384%\n",
      "(32.24373584613204, 0.97178)\n",
      "Testing:\n",
      "\t0/79: loss = 0.5369, Acc = 89.062%\n",
      "\n",
      "===> epoch: 18/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0180, Acc = 100.000%\n",
      "\t99/391: loss = 0.0683, Acc = 97.516%\n",
      "\t198/391: loss = 0.0707, Acc = 97.468%\n",
      "\t297/391: loss = 0.0739, Acc = 97.339%\n",
      "(28.807159814983606, 0.97362)\n",
      "Testing:\n",
      "\t0/79: loss = 0.5721, Acc = 86.719%\n",
      "\n",
      "===> epoch: 19/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0168, Acc = 100.000%\n",
      "\t99/391: loss = 0.0534, Acc = 98.164%\n",
      "\t198/391: loss = 0.0603, Acc = 97.817%\n",
      "\t297/391: loss = 0.0650, Acc = 97.675%\n",
      "(26.269520295783877, 0.97652)\n",
      "Testing:\n",
      "\t0/79: loss = 0.5239, Acc = 87.500%\n",
      "\n",
      "===> epoch: 20/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0351, Acc = 99.219%\n",
      "\t99/391: loss = 0.0523, Acc = 98.141%\n",
      "\t198/391: loss = 0.0548, Acc = 98.108%\n",
      "\t297/391: loss = 0.0578, Acc = 97.997%\n",
      "(23.95281110610813, 0.979)\n",
      "Testing:\n",
      "\t0/79: loss = 0.5111, Acc = 89.844%\n",
      "\n",
      "===> epoch: 21/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0412, Acc = 98.438%\n",
      "\t99/391: loss = 0.0479, Acc = 98.305%\n",
      "\t198/391: loss = 0.0472, Acc = 98.394%\n",
      "\t297/391: loss = 0.0522, Acc = 98.188%\n",
      "(21.30974343419075, 0.9811)\n",
      "Testing:\n",
      "\t0/79: loss = 0.5306, Acc = 85.938%\n",
      "\n",
      "===> epoch: 22/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0757, Acc = 96.094%\n",
      "\t99/391: loss = 0.0440, Acc = 98.500%\n",
      "\t198/391: loss = 0.0460, Acc = 98.500%\n",
      "\t297/391: loss = 0.0509, Acc = 98.288%\n",
      "(19.606107980944216, 0.98312)\n",
      "Testing:\n",
      "\t0/79: loss = 0.5373, Acc = 87.500%\n",
      "\n",
      "===> epoch: 23/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0421, Acc = 99.219%\n",
      "\t99/391: loss = 0.0331, Acc = 98.891%\n",
      "\t198/391: loss = 0.0378, Acc = 98.752%\n",
      "\t297/391: loss = 0.0490, Acc = 98.424%\n",
      "(19.533574868924916, 0.98372)\n",
      "Testing:\n",
      "\t0/79: loss = 0.5367, Acc = 88.281%\n",
      "\n",
      "===> epoch: 24/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0399, Acc = 98.438%\n",
      "\t99/391: loss = 0.0394, Acc = 98.641%\n",
      "\t198/391: loss = 0.0451, Acc = 98.351%\n",
      "\t297/391: loss = 0.0459, Acc = 98.356%\n",
      "(18.119870732538402, 0.98348)\n",
      "Testing:\n",
      "\t0/79: loss = 0.4594, Acc = 86.719%\n",
      "\n",
      "===> epoch: 25/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0062, Acc = 100.000%\n",
      "\t99/391: loss = 0.0320, Acc = 98.906%\n",
      "\t198/391: loss = 0.0417, Acc = 98.606%\n",
      "\t297/391: loss = 0.0444, Acc = 98.519%\n",
      "(18.232849354855716, 0.98452)\n",
      "Testing:\n",
      "\t0/79: loss = 0.5843, Acc = 85.938%\n",
      "\n",
      "===> epoch: 26/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0516, Acc = 98.438%\n",
      "\t99/391: loss = 0.0393, Acc = 98.641%\n",
      "\t198/391: loss = 0.0412, Acc = 98.587%\n",
      "\t297/391: loss = 0.0412, Acc = 98.590%\n",
      "(16.76977751031518, 0.98506)\n",
      "Testing:\n",
      "\t0/79: loss = 0.5035, Acc = 88.281%\n",
      "\n",
      "===> epoch: 27/50\n",
      "Training:\n",
      "\t0/391: loss = 0.1168, Acc = 96.094%\n",
      "\t99/391: loss = 0.0430, Acc = 98.547%\n",
      "\t198/391: loss = 0.0435, Acc = 98.606%\n",
      "\t297/391: loss = 0.0427, Acc = 98.605%\n",
      "(16.42813345324248, 0.98618)\n",
      "Testing:\n",
      "\t0/79: loss = 0.5334, Acc = 87.500%\n",
      "\n",
      "===> epoch: 28/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0467, Acc = 98.438%\n",
      "\t99/391: loss = 0.0349, Acc = 98.648%\n",
      "\t198/391: loss = 0.0408, Acc = 98.465%\n",
      "\t297/391: loss = 0.0388, Acc = 98.563%\n",
      "(16.22019277047366, 0.98474)\n",
      "Testing:\n",
      "\t0/79: loss = 0.6209, Acc = 84.375%\n",
      "\n",
      "===> epoch: 29/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0952, Acc = 97.656%\n",
      "\t99/391: loss = 0.0346, Acc = 98.906%\n",
      "\t198/391: loss = 0.0348, Acc = 98.877%\n",
      "\t297/391: loss = 0.0357, Acc = 98.815%\n",
      "(14.610630338080227, 0.98748)\n",
      "Testing:\n",
      "\t0/79: loss = 0.5451, Acc = 86.719%\n",
      "\n",
      "===> epoch: 30/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0978, Acc = 97.656%\n",
      "\t99/391: loss = 0.0302, Acc = 99.031%\n",
      "\t198/391: loss = 0.0307, Acc = 98.999%\n",
      "\t297/391: loss = 0.0329, Acc = 98.912%\n",
      "(13.22671419288963, 0.9886)\n",
      "Testing:\n",
      "\t0/79: loss = 0.4021, Acc = 89.062%\n",
      "\n",
      "===> epoch: 31/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0130, Acc = 99.219%\n",
      "\t99/391: loss = 0.0419, Acc = 98.430%\n",
      "\t198/391: loss = 0.0380, Acc = 98.614%\n",
      "\t297/391: loss = 0.0348, Acc = 98.768%\n",
      "(13.37581449886784, 0.98792)\n",
      "Testing:\n",
      "\t0/79: loss = 0.5981, Acc = 85.156%\n",
      "\n",
      "===> epoch: 32/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0601, Acc = 97.656%\n",
      "\t99/391: loss = 0.0334, Acc = 98.812%\n",
      "\t198/391: loss = 0.0306, Acc = 98.944%\n",
      "\t297/391: loss = 0.0314, Acc = 98.938%\n",
      "(13.010154109448195, 0.98874)\n",
      "Testing:\n",
      "\t0/79: loss = 0.7379, Acc = 83.594%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> epoch: 33/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0169, Acc = 100.000%\n",
      "\t99/391: loss = 0.0381, Acc = 98.789%\n",
      "\t198/391: loss = 0.0366, Acc = 98.818%\n",
      "\t297/391: loss = 0.0335, Acc = 98.904%\n",
      "(12.824751075357199, 0.98934)\n",
      "Testing:\n",
      "\t0/79: loss = 0.4120, Acc = 89.062%\n",
      "\n",
      "===> epoch: 34/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0217, Acc = 99.219%\n",
      "\t99/391: loss = 0.0223, Acc = 99.281%\n",
      "\t198/391: loss = 0.0264, Acc = 99.144%\n",
      "\t297/391: loss = 0.0266, Acc = 99.156%\n",
      "(11.337791154161096, 0.99054)\n",
      "Testing:\n",
      "\t0/79: loss = 0.7190, Acc = 86.719%\n",
      "\n",
      "===> epoch: 35/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0167, Acc = 100.000%\n",
      "\t99/391: loss = 0.0263, Acc = 99.070%\n",
      "\t198/391: loss = 0.0288, Acc = 98.999%\n",
      "\t297/391: loss = 0.0307, Acc = 98.954%\n",
      "(11.794409663416445, 0.98978)\n",
      "Testing:\n",
      "\t0/79: loss = 0.3620, Acc = 92.188%\n",
      "\n",
      "===> epoch: 36/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0269, Acc = 100.000%\n",
      "\t99/391: loss = 0.0276, Acc = 99.070%\n",
      "\t198/391: loss = 0.0297, Acc = 99.022%\n",
      "\t297/391: loss = 0.0284, Acc = 99.056%\n",
      "(11.183280417695642, 0.99044)\n",
      "Testing:\n",
      "\t0/79: loss = 0.5387, Acc = 87.500%\n",
      "\n",
      "===> epoch: 37/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0021, Acc = 100.000%\n",
      "\t99/391: loss = 0.0199, Acc = 99.297%\n",
      "\t198/391: loss = 0.0201, Acc = 99.274%\n",
      "\t297/391: loss = 0.0251, Acc = 99.135%\n",
      "(11.814019217155874, 0.98946)\n",
      "Testing:\n",
      "\t0/79: loss = 0.5362, Acc = 87.500%\n",
      "\n",
      "===> epoch: 38/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0083, Acc = 100.000%\n",
      "\t99/391: loss = 0.0268, Acc = 99.062%\n",
      "\t198/391: loss = 0.0267, Acc = 99.081%\n",
      "\t297/391: loss = 0.0294, Acc = 98.972%\n",
      "(11.906339340377599, 0.98922)\n",
      "Testing:\n",
      "\t0/79: loss = 0.5845, Acc = 88.281%\n",
      "\n",
      "===> epoch: 39/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0041, Acc = 100.000%\n",
      "\t99/391: loss = 0.0214, Acc = 99.227%\n",
      "\t198/391: loss = 0.0208, Acc = 99.309%\n",
      "\t297/391: loss = 0.0222, Acc = 99.237%\n",
      "(9.195571181364357, 0.99186)\n",
      "Testing:\n",
      "\t0/79: loss = 0.4572, Acc = 88.281%\n",
      "\n",
      "===> epoch: 40/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0072, Acc = 100.000%\n",
      "\t99/391: loss = 0.0188, Acc = 99.359%\n",
      "\t198/391: loss = 0.0224, Acc = 99.258%\n",
      "\t297/391: loss = 0.0239, Acc = 99.185%\n",
      "(9.216838746331632, 0.9919)\n",
      "Testing:\n",
      "\t0/79: loss = 0.4215, Acc = 90.625%\n",
      "\n",
      "===> epoch: 41/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0018, Acc = 100.000%\n",
      "\t99/391: loss = 0.0232, Acc = 99.281%\n",
      "\t198/391: loss = 0.0217, Acc = 99.340%\n",
      "\t297/391: loss = 0.0252, Acc = 99.177%\n",
      "(10.323638565139845, 0.99114)\n",
      "Testing:\n",
      "\t0/79: loss = 0.6866, Acc = 88.281%\n",
      "\n",
      "===> epoch: 42/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0300, Acc = 99.219%\n",
      "\t99/391: loss = 0.0236, Acc = 99.227%\n",
      "\t198/391: loss = 0.0222, Acc = 99.266%\n",
      "\t297/391: loss = 0.0236, Acc = 99.250%\n",
      "(9.826867204159498, 0.99194)\n",
      "Testing:\n",
      "\t0/79: loss = 0.4076, Acc = 89.062%\n",
      "\n",
      "===> epoch: 43/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0086, Acc = 100.000%\n",
      "\t99/391: loss = 0.0233, Acc = 99.211%\n",
      "\t198/391: loss = 0.0225, Acc = 99.242%\n",
      "\t297/391: loss = 0.0245, Acc = 99.156%\n",
      "(10.022283032536507, 0.99134)\n",
      "Testing:\n",
      "\t0/79: loss = 0.3421, Acc = 92.969%\n",
      "\n",
      "===> epoch: 44/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0063, Acc = 100.000%\n",
      "\t99/391: loss = 0.0244, Acc = 99.227%\n",
      "\t198/391: loss = 0.0233, Acc = 99.231%\n",
      "\t297/391: loss = 0.0232, Acc = 99.242%\n",
      "(9.33742645336315, 0.99192)\n",
      "Testing:\n",
      "\t0/79: loss = 0.5264, Acc = 88.281%\n",
      "\n",
      "===> epoch: 45/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0513, Acc = 99.219%\n",
      "\t99/391: loss = 0.0200, Acc = 99.289%\n",
      "\t198/391: loss = 0.0199, Acc = 99.321%\n",
      "\t297/391: loss = 0.0205, Acc = 99.258%\n",
      "(8.397287604864687, 0.99234)\n",
      "Testing:\n",
      "\t0/79: loss = 0.5145, Acc = 89.062%\n",
      "\n",
      "===> epoch: 46/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0037, Acc = 100.000%\n",
      "\t99/391: loss = 0.0205, Acc = 99.375%\n",
      "\t198/391: loss = 0.0207, Acc = 99.340%\n",
      "\t297/391: loss = 0.0210, Acc = 99.287%\n",
      "(8.768965712748468, 0.99244)\n",
      "Testing:\n",
      "\t0/79: loss = 0.4456, Acc = 89.844%\n",
      "\n",
      "===> epoch: 47/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0105, Acc = 99.219%\n",
      "\t99/391: loss = 0.0224, Acc = 99.195%\n",
      "\t198/391: loss = 0.0212, Acc = 99.234%\n",
      "\t297/391: loss = 0.0260, Acc = 99.093%\n",
      "(9.62723974417895, 0.99142)\n",
      "Testing:\n",
      "\t0/79: loss = 0.4712, Acc = 87.500%\n",
      "\n",
      "===> epoch: 48/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0035, Acc = 100.000%\n",
      "\t99/391: loss = 0.0210, Acc = 99.289%\n",
      "\t198/391: loss = 0.0211, Acc = 99.313%\n",
      "\t297/391: loss = 0.0198, Acc = 99.337%\n",
      "(7.222689769696444, 0.99374)\n",
      "Testing:\n",
      "\t0/79: loss = 0.4460, Acc = 89.062%\n",
      "\n",
      "===> epoch: 49/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0145, Acc = 99.219%\n",
      "\t99/391: loss = 0.0171, Acc = 99.461%\n",
      "\t198/391: loss = 0.0179, Acc = 99.415%\n",
      "\t297/391: loss = 0.0232, Acc = 99.234%\n",
      "(9.248200234258547, 0.9925)\n",
      "Testing:\n",
      "\t0/79: loss = 0.4850, Acc = 89.844%\n",
      "\n",
      "===> epoch: 50/50\n",
      "Training:\n",
      "\t0/391: loss = 0.0011, Acc = 100.000%\n",
      "\t99/391: loss = 0.0150, Acc = 99.500%\n",
      "\t198/391: loss = 0.0155, Acc = 99.454%\n",
      "\t297/391: loss = 0.0160, Acc = 99.452%\n",
      "(7.466169737745076, 0.99382)\n",
      "Testing:\n",
      "\t0/79: loss = 0.4842, Acc = 89.844%\n",
      "===> BEST ACC. PERFORMANCE: 86.390%\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Solver' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-c7bc734e5e35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-6a0d3bc5eb38>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mCLASSES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'plane'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'car'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bird'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'frog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'horse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ship'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'truck'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-6a0d3bc5eb38>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"===> BEST ACC. PERFORMANCE: {:.3f}%\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Solver' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
