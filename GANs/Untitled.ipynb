{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-02T09:22:13.631933Z",
     "start_time": "2019-05-02T09:21:34.194739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(b1=0.5, b2=0.999, batch_size=64, channels=1, img_size=32, latent_dim=100, lr=0.0002, n_classes=10, n_cpu=8, n_epochs=50, sample_interval=400)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaliu/Dev/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
      "/home/yaliu/Dev/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/50] [Batch 0/938] [D loss: 1.497996, acc: 11%] [G loss: 1.490640]\n",
      "[Epoch 0/50] [Batch 1/938] [D loss: 1.497958, acc: 9%] [G loss: 1.490550]\n",
      "[Epoch 0/50] [Batch 2/938] [D loss: 1.497775, acc: 10%] [G loss: 1.490907]\n",
      "[Epoch 0/50] [Batch 3/938] [D loss: 1.498048, acc: 10%] [G loss: 1.491552]\n",
      "[Epoch 0/50] [Batch 4/938] [D loss: 1.497958, acc: 6%] [G loss: 1.491710]\n",
      "[Epoch 0/50] [Batch 5/938] [D loss: 1.497511, acc: 10%] [G loss: 1.491505]\n",
      "[Epoch 0/50] [Batch 6/938] [D loss: 1.498003, acc: 7%] [G loss: 1.492139]\n",
      "[Epoch 0/50] [Batch 7/938] [D loss: 1.497864, acc: 13%] [G loss: 1.492322]\n",
      "[Epoch 0/50] [Batch 8/938] [D loss: 1.497711, acc: 10%] [G loss: 1.492143]\n",
      "[Epoch 0/50] [Batch 9/938] [D loss: 1.497556, acc: 10%] [G loss: 1.492333]\n",
      "[Epoch 0/50] [Batch 10/938] [D loss: 1.497551, acc: 12%] [G loss: 1.492511]\n",
      "[Epoch 0/50] [Batch 11/938] [D loss: 1.497652, acc: 9%] [G loss: 1.493167]\n",
      "[Epoch 0/50] [Batch 12/938] [D loss: 1.497694, acc: 10%] [G loss: 1.493444]\n",
      "[Epoch 0/50] [Batch 13/938] [D loss: 1.497478, acc: 7%] [G loss: 1.493357]\n",
      "[Epoch 0/50] [Batch 14/938] [D loss: 1.497293, acc: 10%] [G loss: 1.493515]\n",
      "[Epoch 0/50] [Batch 15/938] [D loss: 1.497103, acc: 10%] [G loss: 1.493449]\n",
      "[Epoch 0/50] [Batch 16/938] [D loss: 1.497019, acc: 12%] [G loss: 1.493848]\n",
      "[Epoch 0/50] [Batch 17/938] [D loss: 1.497358, acc: 7%] [G loss: 1.493907]\n",
      "[Epoch 0/50] [Batch 18/938] [D loss: 1.497310, acc: 10%] [G loss: 1.493734]\n",
      "[Epoch 0/50] [Batch 19/938] [D loss: 1.497417, acc: 8%] [G loss: 1.493544]\n",
      "[Epoch 0/50] [Batch 20/938] [D loss: 1.496865, acc: 7%] [G loss: 1.493611]\n",
      "[Epoch 0/50] [Batch 21/938] [D loss: 1.497025, acc: 13%] [G loss: 1.492608]\n",
      "[Epoch 0/50] [Batch 22/938] [D loss: 1.496922, acc: 11%] [G loss: 1.492749]\n",
      "[Epoch 0/50] [Batch 23/938] [D loss: 1.496788, acc: 14%] [G loss: 1.491823]\n",
      "[Epoch 0/50] [Batch 24/938] [D loss: 1.496771, acc: 8%] [G loss: 1.491074]\n",
      "[Epoch 0/50] [Batch 25/938] [D loss: 1.496508, acc: 9%] [G loss: 1.491658]\n",
      "[Epoch 0/50] [Batch 26/938] [D loss: 1.497026, acc: 8%] [G loss: 1.490249]\n",
      "[Epoch 0/50] [Batch 27/938] [D loss: 1.496712, acc: 14%] [G loss: 1.489777]\n",
      "[Epoch 0/50] [Batch 28/938] [D loss: 1.496197, acc: 7%] [G loss: 1.491541]\n",
      "[Epoch 0/50] [Batch 29/938] [D loss: 1.496552, acc: 6%] [G loss: 1.490330]\n",
      "[Epoch 0/50] [Batch 30/938] [D loss: 1.496616, acc: 10%] [G loss: 1.490104]\n",
      "[Epoch 0/50] [Batch 31/938] [D loss: 1.496609, acc: 8%] [G loss: 1.490902]\n",
      "[Epoch 0/50] [Batch 32/938] [D loss: 1.496421, acc: 4%] [G loss: 1.490499]\n",
      "[Epoch 0/50] [Batch 33/938] [D loss: 1.496135, acc: 12%] [G loss: 1.492499]\n",
      "[Epoch 0/50] [Batch 34/938] [D loss: 1.496059, acc: 7%] [G loss: 1.493241]\n",
      "[Epoch 0/50] [Batch 35/938] [D loss: 1.495571, acc: 8%] [G loss: 1.492966]\n",
      "[Epoch 0/50] [Batch 36/938] [D loss: 1.494889, acc: 11%] [G loss: 1.496319]\n",
      "[Epoch 0/50] [Batch 37/938] [D loss: 1.495030, acc: 7%] [G loss: 1.497331]\n",
      "[Epoch 0/50] [Batch 38/938] [D loss: 1.494103, acc: 13%] [G loss: 1.497576]\n",
      "[Epoch 0/50] [Batch 39/938] [D loss: 1.493909, acc: 10%] [G loss: 1.496939]\n",
      "[Epoch 0/50] [Batch 40/938] [D loss: 1.494274, acc: 11%] [G loss: 1.499588]\n",
      "[Epoch 0/50] [Batch 41/938] [D loss: 1.494838, acc: 8%] [G loss: 1.497653]\n",
      "[Epoch 0/50] [Batch 42/938] [D loss: 1.495327, acc: 13%] [G loss: 1.495272]\n",
      "[Epoch 0/50] [Batch 43/938] [D loss: 1.494533, acc: 11%] [G loss: 1.491367]\n",
      "[Epoch 0/50] [Batch 44/938] [D loss: 1.494273, acc: 14%] [G loss: 1.491376]\n",
      "[Epoch 0/50] [Batch 45/938] [D loss: 1.495241, acc: 9%] [G loss: 1.486968]\n",
      "[Epoch 0/50] [Batch 46/938] [D loss: 1.493100, acc: 10%] [G loss: 1.487773]\n",
      "[Epoch 0/50] [Batch 47/938] [D loss: 1.491012, acc: 13%] [G loss: 1.487515]\n",
      "[Epoch 0/50] [Batch 48/938] [D loss: 1.489721, acc: 10%] [G loss: 1.483492]\n",
      "[Epoch 0/50] [Batch 49/938] [D loss: 1.490977, acc: 10%] [G loss: 1.480520]\n",
      "[Epoch 0/50] [Batch 50/938] [D loss: 1.489203, acc: 10%] [G loss: 1.477810]\n",
      "[Epoch 0/50] [Batch 51/938] [D loss: 1.490477, acc: 10%] [G loss: 1.472449]\n",
      "[Epoch 0/50] [Batch 52/938] [D loss: 1.490501, acc: 10%] [G loss: 1.472840]\n",
      "[Epoch 0/50] [Batch 53/938] [D loss: 1.497789, acc: 10%] [G loss: 1.467647]\n",
      "[Epoch 0/50] [Batch 54/938] [D loss: 1.493446, acc: 8%] [G loss: 1.467371]\n",
      "[Epoch 0/50] [Batch 55/938] [D loss: 1.496908, acc: 12%] [G loss: 1.470903]\n",
      "[Epoch 0/50] [Batch 56/938] [D loss: 1.497809, acc: 8%] [G loss: 1.469193]\n",
      "[Epoch 0/50] [Batch 57/938] [D loss: 1.498277, acc: 10%] [G loss: 1.477467]\n",
      "[Epoch 0/50] [Batch 58/938] [D loss: 1.496942, acc: 16%] [G loss: 1.486405]\n",
      "[Epoch 0/50] [Batch 59/938] [D loss: 1.495547, acc: 12%] [G loss: 1.490087]\n",
      "[Epoch 0/50] [Batch 60/938] [D loss: 1.495777, acc: 13%] [G loss: 1.496942]\n",
      "[Epoch 0/50] [Batch 61/938] [D loss: 1.497120, acc: 7%] [G loss: 1.500694]\n",
      "[Epoch 0/50] [Batch 62/938] [D loss: 1.498725, acc: 14%] [G loss: 1.507325]\n",
      "[Epoch 0/50] [Batch 63/938] [D loss: 1.497557, acc: 17%] [G loss: 1.502607]\n",
      "[Epoch 0/50] [Batch 64/938] [D loss: 1.493971, acc: 8%] [G loss: 1.509473]\n",
      "[Epoch 0/50] [Batch 65/938] [D loss: 1.496886, acc: 10%] [G loss: 1.505214]\n",
      "[Epoch 0/50] [Batch 66/938] [D loss: 1.498562, acc: 10%] [G loss: 1.501404]\n",
      "[Epoch 0/50] [Batch 67/938] [D loss: 1.499733, acc: 16%] [G loss: 1.499060]\n",
      "[Epoch 0/50] [Batch 68/938] [D loss: 1.498030, acc: 14%] [G loss: 1.501015]\n",
      "[Epoch 0/50] [Batch 69/938] [D loss: 1.499574, acc: 14%] [G loss: 1.497511]\n",
      "[Epoch 0/50] [Batch 70/938] [D loss: 1.502979, acc: 14%] [G loss: 1.496298]\n",
      "[Epoch 0/50] [Batch 71/938] [D loss: 1.500087, acc: 17%] [G loss: 1.494805]\n",
      "[Epoch 0/50] [Batch 72/938] [D loss: 1.497321, acc: 10%] [G loss: 1.498008]\n",
      "[Epoch 0/50] [Batch 73/938] [D loss: 1.494005, acc: 10%] [G loss: 1.495291]\n",
      "[Epoch 0/50] [Batch 74/938] [D loss: 1.492431, acc: 10%] [G loss: 1.490129]\n",
      "[Epoch 0/50] [Batch 75/938] [D loss: 1.494142, acc: 10%] [G loss: 1.488203]\n",
      "[Epoch 0/50] [Batch 76/938] [D loss: 1.493284, acc: 15%] [G loss: 1.483091]\n",
      "[Epoch 0/50] [Batch 77/938] [D loss: 1.492936, acc: 10%] [G loss: 1.474009]\n",
      "[Epoch 0/50] [Batch 78/938] [D loss: 1.496696, acc: 13%] [G loss: 1.467561]\n",
      "[Epoch 0/50] [Batch 79/938] [D loss: 1.502304, acc: 14%] [G loss: 1.464675]\n",
      "[Epoch 0/50] [Batch 80/938] [D loss: 1.498252, acc: 14%] [G loss: 1.469326]\n",
      "[Epoch 0/50] [Batch 81/938] [D loss: 1.504447, acc: 16%] [G loss: 1.469710]\n",
      "[Epoch 0/50] [Batch 82/938] [D loss: 1.500525, acc: 9%] [G loss: 1.483756]\n",
      "[Epoch 0/50] [Batch 83/938] [D loss: 1.501097, acc: 10%] [G loss: 1.487124]\n",
      "[Epoch 0/50] [Batch 84/938] [D loss: 1.499831, acc: 10%] [G loss: 1.500592]\n",
      "[Epoch 0/50] [Batch 85/938] [D loss: 1.499722, acc: 8%] [G loss: 1.498307]\n",
      "[Epoch 0/50] [Batch 86/938] [D loss: 1.498486, acc: 8%] [G loss: 1.505345]\n",
      "[Epoch 0/50] [Batch 87/938] [D loss: 1.496440, acc: 12%] [G loss: 1.510747]\n",
      "[Epoch 0/50] [Batch 88/938] [D loss: 1.495024, acc: 11%] [G loss: 1.515504]\n",
      "[Epoch 0/50] [Batch 89/938] [D loss: 1.495543, acc: 10%] [G loss: 1.518316]\n",
      "[Epoch 0/50] [Batch 90/938] [D loss: 1.497782, acc: 15%] [G loss: 1.515551]\n",
      "[Epoch 0/50] [Batch 91/938] [D loss: 1.493791, acc: 8%] [G loss: 1.518802]\n",
      "[Epoch 0/50] [Batch 92/938] [D loss: 1.493616, acc: 17%] [G loss: 1.517893]\n",
      "[Epoch 0/50] [Batch 93/938] [D loss: 1.493747, acc: 14%] [G loss: 1.520187]\n",
      "[Epoch 0/50] [Batch 94/938] [D loss: 1.498672, acc: 10%] [G loss: 1.514553]\n",
      "[Epoch 0/50] [Batch 95/938] [D loss: 1.498302, acc: 14%] [G loss: 1.513028]\n",
      "[Epoch 0/50] [Batch 96/938] [D loss: 1.501691, acc: 6%] [G loss: 1.503726]\n",
      "[Epoch 0/50] [Batch 97/938] [D loss: 1.495811, acc: 20%] [G loss: 1.504815]\n",
      "[Epoch 0/50] [Batch 98/938] [D loss: 1.502366, acc: 9%] [G loss: 1.497543]\n",
      "[Epoch 0/50] [Batch 99/938] [D loss: 1.500919, acc: 13%] [G loss: 1.494622]\n",
      "[Epoch 0/50] [Batch 100/938] [D loss: 1.501316, acc: 17%] [G loss: 1.490652]\n",
      "[Epoch 0/50] [Batch 101/938] [D loss: 1.500931, acc: 10%] [G loss: 1.491469]\n",
      "[Epoch 0/50] [Batch 102/938] [D loss: 1.495524, acc: 13%] [G loss: 1.492839]\n",
      "[Epoch 0/50] [Batch 103/938] [D loss: 1.494696, acc: 16%] [G loss: 1.492877]\n",
      "[Epoch 0/50] [Batch 104/938] [D loss: 1.492440, acc: 13%] [G loss: 1.492831]\n",
      "[Epoch 0/50] [Batch 105/938] [D loss: 1.489615, acc: 14%] [G loss: 1.488155]\n",
      "[Epoch 0/50] [Batch 106/938] [D loss: 1.492564, acc: 11%] [G loss: 1.483994]\n",
      "[Epoch 0/50] [Batch 107/938] [D loss: 1.490564, acc: 22%] [G loss: 1.479451]\n",
      "[Epoch 0/50] [Batch 108/938] [D loss: 1.501183, acc: 17%] [G loss: 1.471484]\n",
      "[Epoch 0/50] [Batch 109/938] [D loss: 1.500615, acc: 16%] [G loss: 1.470241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/50] [Batch 110/938] [D loss: 1.499318, acc: 11%] [G loss: 1.466677]\n",
      "[Epoch 0/50] [Batch 111/938] [D loss: 1.502099, acc: 10%] [G loss: 1.468029]\n",
      "[Epoch 0/50] [Batch 112/938] [D loss: 1.502769, acc: 22%] [G loss: 1.472178]\n",
      "[Epoch 0/50] [Batch 113/938] [D loss: 1.503663, acc: 10%] [G loss: 1.477081]\n",
      "[Epoch 0/50] [Batch 114/938] [D loss: 1.499975, acc: 13%] [G loss: 1.486194]\n",
      "[Epoch 0/50] [Batch 115/938] [D loss: 1.499349, acc: 11%] [G loss: 1.493463]\n",
      "[Epoch 0/50] [Batch 116/938] [D loss: 1.501559, acc: 14%] [G loss: 1.495735]\n",
      "[Epoch 0/50] [Batch 117/938] [D loss: 1.499971, acc: 11%] [G loss: 1.503789]\n",
      "[Epoch 0/50] [Batch 118/938] [D loss: 1.497524, acc: 10%] [G loss: 1.509943]\n",
      "[Epoch 0/50] [Batch 119/938] [D loss: 1.500595, acc: 18%] [G loss: 1.512823]\n",
      "[Epoch 0/50] [Batch 120/938] [D loss: 1.496360, acc: 10%] [G loss: 1.515543]\n",
      "[Epoch 0/50] [Batch 121/938] [D loss: 1.493623, acc: 20%] [G loss: 1.520279]\n",
      "[Epoch 0/50] [Batch 122/938] [D loss: 1.496781, acc: 16%] [G loss: 1.518486]\n",
      "[Epoch 0/50] [Batch 123/938] [D loss: 1.494335, acc: 21%] [G loss: 1.520816]\n",
      "[Epoch 0/50] [Batch 124/938] [D loss: 1.493441, acc: 13%] [G loss: 1.521443]\n",
      "[Epoch 0/50] [Batch 125/938] [D loss: 1.494794, acc: 19%] [G loss: 1.521914]\n",
      "[Epoch 0/50] [Batch 126/938] [D loss: 1.494597, acc: 14%] [G loss: 1.520787]\n",
      "[Epoch 0/50] [Batch 127/938] [D loss: 1.494281, acc: 13%] [G loss: 1.516443]\n",
      "[Epoch 0/50] [Batch 128/938] [D loss: 1.494290, acc: 14%] [G loss: 1.518373]\n",
      "[Epoch 0/50] [Batch 129/938] [D loss: 1.494444, acc: 16%] [G loss: 1.514522]\n",
      "[Epoch 0/50] [Batch 130/938] [D loss: 1.500283, acc: 14%] [G loss: 1.519045]\n",
      "[Epoch 0/50] [Batch 131/938] [D loss: 1.498620, acc: 20%] [G loss: 1.505257]\n",
      "[Epoch 0/50] [Batch 132/938] [D loss: 1.499254, acc: 20%] [G loss: 1.502785]\n",
      "[Epoch 0/50] [Batch 133/938] [D loss: 1.500892, acc: 12%] [G loss: 1.500273]\n",
      "[Epoch 0/50] [Batch 134/938] [D loss: 1.501616, acc: 19%] [G loss: 1.497425]\n",
      "[Epoch 0/50] [Batch 135/938] [D loss: 1.500697, acc: 15%] [G loss: 1.492689]\n",
      "[Epoch 0/50] [Batch 136/938] [D loss: 1.498641, acc: 19%] [G loss: 1.492050]\n",
      "[Epoch 0/50] [Batch 137/938] [D loss: 1.500821, acc: 17%] [G loss: 1.494062]\n",
      "[Epoch 0/50] [Batch 138/938] [D loss: 1.497208, acc: 20%] [G loss: 1.493195]\n",
      "[Epoch 0/50] [Batch 139/938] [D loss: 1.495752, acc: 14%] [G loss: 1.493649]\n",
      "[Epoch 0/50] [Batch 140/938] [D loss: 1.493948, acc: 21%] [G loss: 1.494904]\n",
      "[Epoch 0/50] [Batch 141/938] [D loss: 1.495436, acc: 14%] [G loss: 1.491383]\n",
      "[Epoch 0/50] [Batch 142/938] [D loss: 1.492417, acc: 15%] [G loss: 1.491985]\n",
      "[Epoch 0/50] [Batch 143/938] [D loss: 1.494874, acc: 18%] [G loss: 1.489409]\n",
      "[Epoch 0/50] [Batch 144/938] [D loss: 1.492788, acc: 20%] [G loss: 1.482055]\n",
      "[Epoch 0/50] [Batch 145/938] [D loss: 1.492944, acc: 16%] [G loss: 1.482967]\n",
      "[Epoch 0/50] [Batch 146/938] [D loss: 1.495270, acc: 13%] [G loss: 1.479153]\n",
      "[Epoch 0/50] [Batch 147/938] [D loss: 1.495921, acc: 21%] [G loss: 1.483316]\n",
      "[Epoch 0/50] [Batch 148/938] [D loss: 1.499339, acc: 18%] [G loss: 1.477335]\n",
      "[Epoch 0/50] [Batch 149/938] [D loss: 1.499258, acc: 25%] [G loss: 1.479396]\n",
      "[Epoch 0/50] [Batch 150/938] [D loss: 1.499209, acc: 18%] [G loss: 1.482712]\n",
      "[Epoch 0/50] [Batch 151/938] [D loss: 1.503168, acc: 15%] [G loss: 1.481128]\n",
      "[Epoch 0/50] [Batch 152/938] [D loss: 1.499453, acc: 19%] [G loss: 1.494207]\n",
      "[Epoch 0/50] [Batch 153/938] [D loss: 1.499618, acc: 11%] [G loss: 1.498506]\n",
      "[Epoch 0/50] [Batch 154/938] [D loss: 1.500791, acc: 17%] [G loss: 1.495792]\n",
      "[Epoch 0/50] [Batch 155/938] [D loss: 1.496437, acc: 24%] [G loss: 1.504764]\n",
      "[Epoch 0/50] [Batch 156/938] [D loss: 1.500871, acc: 18%] [G loss: 1.502508]\n",
      "[Epoch 0/50] [Batch 157/938] [D loss: 1.496703, acc: 25%] [G loss: 1.510782]\n",
      "[Epoch 0/50] [Batch 158/938] [D loss: 1.496496, acc: 19%] [G loss: 1.510918]\n",
      "[Epoch 0/50] [Batch 159/938] [D loss: 1.498152, acc: 25%] [G loss: 1.512357]\n",
      "[Epoch 0/50] [Batch 160/938] [D loss: 1.496148, acc: 24%] [G loss: 1.516399]\n",
      "[Epoch 0/50] [Batch 161/938] [D loss: 1.493788, acc: 21%] [G loss: 1.517330]\n",
      "[Epoch 0/50] [Batch 162/938] [D loss: 1.494137, acc: 21%] [G loss: 1.516191]\n",
      "[Epoch 0/50] [Batch 163/938] [D loss: 1.495081, acc: 21%] [G loss: 1.515377]\n",
      "[Epoch 0/50] [Batch 164/938] [D loss: 1.497608, acc: 14%] [G loss: 1.516246]\n",
      "[Epoch 0/50] [Batch 165/938] [D loss: 1.497658, acc: 15%] [G loss: 1.513602]\n",
      "[Epoch 0/50] [Batch 166/938] [D loss: 1.496743, acc: 20%] [G loss: 1.511541]\n",
      "[Epoch 0/50] [Batch 167/938] [D loss: 1.498273, acc: 19%] [G loss: 1.507526]\n",
      "[Epoch 0/50] [Batch 168/938] [D loss: 1.498096, acc: 17%] [G loss: 1.502415]\n",
      "[Epoch 0/50] [Batch 169/938] [D loss: 1.498093, acc: 24%] [G loss: 1.501395]\n",
      "[Epoch 0/50] [Batch 170/938] [D loss: 1.498230, acc: 14%] [G loss: 1.498893]\n",
      "[Epoch 0/50] [Batch 171/938] [D loss: 1.494819, acc: 25%] [G loss: 1.497655]\n",
      "[Epoch 0/50] [Batch 172/938] [D loss: 1.492911, acc: 24%] [G loss: 1.495707]\n",
      "[Epoch 0/50] [Batch 173/938] [D loss: 1.494196, acc: 18%] [G loss: 1.492161]\n",
      "[Epoch 0/50] [Batch 174/938] [D loss: 1.493057, acc: 22%] [G loss: 1.488911]\n",
      "[Epoch 0/50] [Batch 175/938] [D loss: 1.495393, acc: 25%] [G loss: 1.483451]\n",
      "[Epoch 0/50] [Batch 176/938] [D loss: 1.494298, acc: 17%] [G loss: 1.479644]\n",
      "[Epoch 0/50] [Batch 177/938] [D loss: 1.494758, acc: 19%] [G loss: 1.475271]\n",
      "[Epoch 0/50] [Batch 178/938] [D loss: 1.501604, acc: 21%] [G loss: 1.465546]\n",
      "[Epoch 0/50] [Batch 179/938] [D loss: 1.501384, acc: 17%] [G loss: 1.466482]\n",
      "[Epoch 0/50] [Batch 180/938] [D loss: 1.500797, acc: 21%] [G loss: 1.464131]\n",
      "[Epoch 0/50] [Batch 181/938] [D loss: 1.502262, acc: 24%] [G loss: 1.471363]\n",
      "[Epoch 0/50] [Batch 182/938] [D loss: 1.505904, acc: 24%] [G loss: 1.470881]\n",
      "[Epoch 0/50] [Batch 183/938] [D loss: 1.499705, acc: 24%] [G loss: 1.478538]\n",
      "[Epoch 0/50] [Batch 184/938] [D loss: 1.500843, acc: 25%] [G loss: 1.484232]\n",
      "[Epoch 0/50] [Batch 185/938] [D loss: 1.500982, acc: 28%] [G loss: 1.492223]\n",
      "[Epoch 0/50] [Batch 186/938] [D loss: 1.497536, acc: 27%] [G loss: 1.498926]\n",
      "[Epoch 0/50] [Batch 187/938] [D loss: 1.496471, acc: 23%] [G loss: 1.498623]\n",
      "[Epoch 0/50] [Batch 188/938] [D loss: 1.497896, acc: 17%] [G loss: 1.503008]\n",
      "[Epoch 0/50] [Batch 189/938] [D loss: 1.496450, acc: 21%] [G loss: 1.504678]\n",
      "[Epoch 0/50] [Batch 190/938] [D loss: 1.495847, acc: 26%] [G loss: 1.509103]\n",
      "[Epoch 0/50] [Batch 191/938] [D loss: 1.495514, acc: 31%] [G loss: 1.510249]\n",
      "[Epoch 0/50] [Batch 192/938] [D loss: 1.494521, acc: 28%] [G loss: 1.509643]\n",
      "[Epoch 0/50] [Batch 193/938] [D loss: 1.492813, acc: 26%] [G loss: 1.512022]\n",
      "[Epoch 0/50] [Batch 194/938] [D loss: 1.494390, acc: 22%] [G loss: 1.508104]\n",
      "[Epoch 0/50] [Batch 195/938] [D loss: 1.495404, acc: 24%] [G loss: 1.509158]\n",
      "[Epoch 0/50] [Batch 196/938] [D loss: 1.492668, acc: 28%] [G loss: 1.508433]\n",
      "[Epoch 0/50] [Batch 197/938] [D loss: 1.493548, acc: 18%] [G loss: 1.508495]\n",
      "[Epoch 0/50] [Batch 198/938] [D loss: 1.494365, acc: 25%] [G loss: 1.508070]\n",
      "[Epoch 0/50] [Batch 199/938] [D loss: 1.497437, acc: 17%] [G loss: 1.502771]\n",
      "[Epoch 0/50] [Batch 200/938] [D loss: 1.497096, acc: 26%] [G loss: 1.503010]\n",
      "[Epoch 0/50] [Batch 201/938] [D loss: 1.494746, acc: 22%] [G loss: 1.500048]\n",
      "[Epoch 0/50] [Batch 202/938] [D loss: 1.494096, acc: 26%] [G loss: 1.500106]\n",
      "[Epoch 0/50] [Batch 203/938] [D loss: 1.498089, acc: 24%] [G loss: 1.498140]\n",
      "[Epoch 0/50] [Batch 204/938] [D loss: 1.496728, acc: 25%] [G loss: 1.501263]\n",
      "[Epoch 0/50] [Batch 205/938] [D loss: 1.493942, acc: 29%] [G loss: 1.492284]\n",
      "[Epoch 0/50] [Batch 206/938] [D loss: 1.497215, acc: 28%] [G loss: 1.491196]\n",
      "[Epoch 0/50] [Batch 207/938] [D loss: 1.494998, acc: 29%] [G loss: 1.488899]\n",
      "[Epoch 0/50] [Batch 208/938] [D loss: 1.496466, acc: 21%] [G loss: 1.493188]\n",
      "[Epoch 0/50] [Batch 209/938] [D loss: 1.494597, acc: 22%] [G loss: 1.492950]\n",
      "[Epoch 0/50] [Batch 210/938] [D loss: 1.492589, acc: 27%] [G loss: 1.492859]\n",
      "[Epoch 0/50] [Batch 211/938] [D loss: 1.491201, acc: 30%] [G loss: 1.494501]\n",
      "[Epoch 0/50] [Batch 212/938] [D loss: 1.493594, acc: 25%] [G loss: 1.486786]\n",
      "[Epoch 0/50] [Batch 213/938] [D loss: 1.493480, acc: 28%] [G loss: 1.481841]\n",
      "[Epoch 0/50] [Batch 214/938] [D loss: 1.499150, acc: 26%] [G loss: 1.480563]\n",
      "[Epoch 0/50] [Batch 215/938] [D loss: 1.498685, acc: 24%] [G loss: 1.485282]\n",
      "[Epoch 0/50] [Batch 216/938] [D loss: 1.496453, acc: 29%] [G loss: 1.486279]\n",
      "[Epoch 0/50] [Batch 217/938] [D loss: 1.495414, acc: 23%] [G loss: 1.491343]\n",
      "[Epoch 0/50] [Batch 218/938] [D loss: 1.501809, acc: 22%] [G loss: 1.486845]\n",
      "[Epoch 0/50] [Batch 219/938] [D loss: 1.500188, acc: 28%] [G loss: 1.491976]\n",
      "[Epoch 0/50] [Batch 220/938] [D loss: 1.498012, acc: 28%] [G loss: 1.496449]\n",
      "[Epoch 0/50] [Batch 221/938] [D loss: 1.501305, acc: 28%] [G loss: 1.497535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/50] [Batch 222/938] [D loss: 1.500472, acc: 26%] [G loss: 1.497333]\n",
      "[Epoch 0/50] [Batch 223/938] [D loss: 1.501295, acc: 24%] [G loss: 1.501004]\n",
      "[Epoch 0/50] [Batch 224/938] [D loss: 1.500174, acc: 26%] [G loss: 1.497547]\n",
      "[Epoch 0/50] [Batch 225/938] [D loss: 1.494352, acc: 26%] [G loss: 1.500774]\n",
      "[Epoch 0/50] [Batch 226/938] [D loss: 1.494701, acc: 30%] [G loss: 1.500471]\n",
      "[Epoch 0/50] [Batch 227/938] [D loss: 1.492400, acc: 31%] [G loss: 1.495899]\n",
      "[Epoch 0/50] [Batch 228/938] [D loss: 1.494085, acc: 28%] [G loss: 1.496784]\n",
      "[Epoch 0/50] [Batch 229/938] [D loss: 1.492884, acc: 22%] [G loss: 1.498771]\n",
      "[Epoch 0/50] [Batch 230/938] [D loss: 1.485852, acc: 31%] [G loss: 1.497828]\n",
      "[Epoch 0/50] [Batch 231/938] [D loss: 1.486344, acc: 34%] [G loss: 1.487892]\n",
      "[Epoch 0/50] [Batch 232/938] [D loss: 1.488444, acc: 24%] [G loss: 1.493075]\n",
      "[Epoch 0/50] [Batch 233/938] [D loss: 1.483617, acc: 32%] [G loss: 1.490319]\n",
      "[Epoch 0/50] [Batch 234/938] [D loss: 1.484059, acc: 32%] [G loss: 1.481113]\n",
      "[Epoch 0/50] [Batch 235/938] [D loss: 1.491900, acc: 31%] [G loss: 1.473909]\n",
      "[Epoch 0/50] [Batch 236/938] [D loss: 1.486806, acc: 29%] [G loss: 1.478680]\n",
      "[Epoch 0/50] [Batch 237/938] [D loss: 1.495497, acc: 28%] [G loss: 1.467188]\n",
      "[Epoch 0/50] [Batch 238/938] [D loss: 1.495964, acc: 28%] [G loss: 1.463368]\n",
      "[Epoch 0/50] [Batch 239/938] [D loss: 1.496609, acc: 27%] [G loss: 1.473617]\n",
      "[Epoch 0/50] [Batch 240/938] [D loss: 1.502932, acc: 28%] [G loss: 1.467612]\n",
      "[Epoch 0/50] [Batch 241/938] [D loss: 1.505593, acc: 28%] [G loss: 1.470475]\n",
      "[Epoch 0/50] [Batch 242/938] [D loss: 1.504047, acc: 25%] [G loss: 1.476707]\n",
      "[Epoch 0/50] [Batch 243/938] [D loss: 1.498515, acc: 27%] [G loss: 1.488991]\n",
      "[Epoch 0/50] [Batch 244/938] [D loss: 1.501762, acc: 30%] [G loss: 1.487106]\n",
      "[Epoch 0/50] [Batch 245/938] [D loss: 1.501001, acc: 33%] [G loss: 1.489924]\n",
      "[Epoch 0/50] [Batch 246/938] [D loss: 1.499101, acc: 32%] [G loss: 1.496886]\n",
      "[Epoch 0/50] [Batch 247/938] [D loss: 1.496765, acc: 31%] [G loss: 1.499563]\n",
      "[Epoch 0/50] [Batch 248/938] [D loss: 1.493065, acc: 35%] [G loss: 1.500098]\n",
      "[Epoch 0/50] [Batch 249/938] [D loss: 1.493378, acc: 28%] [G loss: 1.510890]\n",
      "[Epoch 0/50] [Batch 250/938] [D loss: 1.489999, acc: 23%] [G loss: 1.515403]\n",
      "[Epoch 0/50] [Batch 251/938] [D loss: 1.487749, acc: 32%] [G loss: 1.513602]\n",
      "[Epoch 0/50] [Batch 252/938] [D loss: 1.483751, acc: 29%] [G loss: 1.513214]\n",
      "[Epoch 0/50] [Batch 253/938] [D loss: 1.482578, acc: 25%] [G loss: 1.517126]\n",
      "[Epoch 0/50] [Batch 254/938] [D loss: 1.477267, acc: 27%] [G loss: 1.516298]\n",
      "[Epoch 0/50] [Batch 255/938] [D loss: 1.479175, acc: 31%] [G loss: 1.519056]\n",
      "[Epoch 0/50] [Batch 256/938] [D loss: 1.476341, acc: 37%] [G loss: 1.512297]\n",
      "[Epoch 0/50] [Batch 257/938] [D loss: 1.482837, acc: 28%] [G loss: 1.500304]\n",
      "[Epoch 0/50] [Batch 258/938] [D loss: 1.476851, acc: 28%] [G loss: 1.499865]\n",
      "[Epoch 0/50] [Batch 259/938] [D loss: 1.484564, acc: 35%] [G loss: 1.485473]\n",
      "[Epoch 0/50] [Batch 260/938] [D loss: 1.482091, acc: 34%] [G loss: 1.484467]\n",
      "[Epoch 0/50] [Batch 261/938] [D loss: 1.493825, acc: 28%] [G loss: 1.479425]\n",
      "[Epoch 0/50] [Batch 262/938] [D loss: 1.487993, acc: 31%] [G loss: 1.466846]\n",
      "[Epoch 0/50] [Batch 263/938] [D loss: 1.495003, acc: 25%] [G loss: 1.457739]\n",
      "[Epoch 0/50] [Batch 264/938] [D loss: 1.496975, acc: 28%] [G loss: 1.452500]\n",
      "[Epoch 0/50] [Batch 265/938] [D loss: 1.493382, acc: 26%] [G loss: 1.453110]\n",
      "[Epoch 0/50] [Batch 266/938] [D loss: 1.494076, acc: 25%] [G loss: 1.467591]\n",
      "[Epoch 0/50] [Batch 267/938] [D loss: 1.481594, acc: 32%] [G loss: 1.480729]\n",
      "[Epoch 0/50] [Batch 268/938] [D loss: 1.477525, acc: 35%] [G loss: 1.477370]\n",
      "[Epoch 0/50] [Batch 269/938] [D loss: 1.478609, acc: 27%] [G loss: 1.488363]\n",
      "[Epoch 0/50] [Batch 270/938] [D loss: 1.473495, acc: 27%] [G loss: 1.490208]\n",
      "[Epoch 0/50] [Batch 271/938] [D loss: 1.472078, acc: 26%] [G loss: 1.498051]\n",
      "[Epoch 0/50] [Batch 272/938] [D loss: 1.465360, acc: 37%] [G loss: 1.482736]\n",
      "[Epoch 0/50] [Batch 273/938] [D loss: 1.473389, acc: 31%] [G loss: 1.475881]\n",
      "[Epoch 0/50] [Batch 274/938] [D loss: 1.475012, acc: 22%] [G loss: 1.470512]\n",
      "[Epoch 0/50] [Batch 275/938] [D loss: 1.468164, acc: 25%] [G loss: 1.474261]\n",
      "[Epoch 0/50] [Batch 276/938] [D loss: 1.480247, acc: 29%] [G loss: 1.442281]\n",
      "[Epoch 0/50] [Batch 277/938] [D loss: 1.494660, acc: 27%] [G loss: 1.453779]\n",
      "[Epoch 0/50] [Batch 278/938] [D loss: 1.475909, acc: 28%] [G loss: 1.444235]\n",
      "[Epoch 0/50] [Batch 279/938] [D loss: 1.462536, acc: 35%] [G loss: 1.446527]\n",
      "[Epoch 0/50] [Batch 280/938] [D loss: 1.451592, acc: 39%] [G loss: 1.472400]\n",
      "[Epoch 0/50] [Batch 281/938] [D loss: 1.458026, acc: 28%] [G loss: 1.494586]\n",
      "[Epoch 0/50] [Batch 282/938] [D loss: 1.456989, acc: 27%] [G loss: 1.500581]\n",
      "[Epoch 0/50] [Batch 283/938] [D loss: 1.435053, acc: 27%] [G loss: 1.507738]\n",
      "[Epoch 0/50] [Batch 284/938] [D loss: 1.428517, acc: 29%] [G loss: 1.518252]\n",
      "[Epoch 0/50] [Batch 285/938] [D loss: 1.440836, acc: 26%] [G loss: 1.509754]\n",
      "[Epoch 0/50] [Batch 286/938] [D loss: 1.436621, acc: 25%] [G loss: 1.539594]\n",
      "[Epoch 0/50] [Batch 287/938] [D loss: 1.476094, acc: 21%] [G loss: 1.506081]\n",
      "[Epoch 0/50] [Batch 288/938] [D loss: 1.464447, acc: 22%] [G loss: 1.510357]\n",
      "[Epoch 0/50] [Batch 289/938] [D loss: 1.462428, acc: 34%] [G loss: 1.486493]\n",
      "[Epoch 0/50] [Batch 290/938] [D loss: 1.449722, acc: 35%] [G loss: 1.509022]\n",
      "[Epoch 0/50] [Batch 291/938] [D loss: 1.441090, acc: 28%] [G loss: 1.511097]\n",
      "[Epoch 0/50] [Batch 292/938] [D loss: 1.447952, acc: 28%] [G loss: 1.521542]\n",
      "[Epoch 0/50] [Batch 293/938] [D loss: 1.447521, acc: 28%] [G loss: 1.506768]\n",
      "[Epoch 0/50] [Batch 294/938] [D loss: 1.427004, acc: 33%] [G loss: 1.517090]\n",
      "[Epoch 0/50] [Batch 295/938] [D loss: 1.426057, acc: 32%] [G loss: 1.475427]\n",
      "[Epoch 0/50] [Batch 296/938] [D loss: 1.428737, acc: 25%] [G loss: 1.476496]\n",
      "[Epoch 0/50] [Batch 297/938] [D loss: 1.402756, acc: 35%] [G loss: 1.445610]\n",
      "[Epoch 0/50] [Batch 298/938] [D loss: 1.410511, acc: 36%] [G loss: 1.449933]\n",
      "[Epoch 0/50] [Batch 299/938] [D loss: 1.397934, acc: 35%] [G loss: 1.460190]\n",
      "[Epoch 0/50] [Batch 300/938] [D loss: 1.401097, acc: 35%] [G loss: 1.432158]\n",
      "[Epoch 0/50] [Batch 301/938] [D loss: 1.407592, acc: 35%] [G loss: 1.392669]\n",
      "[Epoch 0/50] [Batch 302/938] [D loss: 1.422445, acc: 34%] [G loss: 1.403551]\n",
      "[Epoch 0/50] [Batch 303/938] [D loss: 1.452612, acc: 28%] [G loss: 1.395702]\n",
      "[Epoch 0/50] [Batch 304/938] [D loss: 1.445068, acc: 33%] [G loss: 1.422482]\n",
      "[Epoch 0/50] [Batch 305/938] [D loss: 1.449128, acc: 29%] [G loss: 1.452762]\n",
      "[Epoch 0/50] [Batch 306/938] [D loss: 1.455084, acc: 32%] [G loss: 1.486659]\n",
      "[Epoch 0/50] [Batch 307/938] [D loss: 1.452993, acc: 34%] [G loss: 1.502051]\n",
      "[Epoch 0/50] [Batch 308/938] [D loss: 1.433251, acc: 34%] [G loss: 1.482780]\n",
      "[Epoch 0/50] [Batch 309/938] [D loss: 1.453064, acc: 30%] [G loss: 1.515785]\n",
      "[Epoch 0/50] [Batch 310/938] [D loss: 1.415932, acc: 38%] [G loss: 1.517497]\n",
      "[Epoch 0/50] [Batch 311/938] [D loss: 1.425363, acc: 35%] [G loss: 1.514708]\n",
      "[Epoch 0/50] [Batch 312/938] [D loss: 1.408038, acc: 39%] [G loss: 1.531851]\n",
      "[Epoch 0/50] [Batch 313/938] [D loss: 1.391647, acc: 36%] [G loss: 1.529796]\n",
      "[Epoch 0/50] [Batch 314/938] [D loss: 1.386402, acc: 43%] [G loss: 1.515552]\n",
      "[Epoch 0/50] [Batch 315/938] [D loss: 1.401033, acc: 40%] [G loss: 1.545768]\n",
      "[Epoch 0/50] [Batch 316/938] [D loss: 1.372420, acc: 40%] [G loss: 1.529315]\n",
      "[Epoch 0/50] [Batch 317/938] [D loss: 1.398318, acc: 38%] [G loss: 1.521796]\n",
      "[Epoch 0/50] [Batch 318/938] [D loss: 1.380871, acc: 44%] [G loss: 1.487571]\n",
      "[Epoch 0/50] [Batch 319/938] [D loss: 1.406116, acc: 37%] [G loss: 1.509845]\n",
      "[Epoch 0/50] [Batch 320/938] [D loss: 1.393807, acc: 40%] [G loss: 1.493732]\n",
      "[Epoch 0/50] [Batch 321/938] [D loss: 1.411004, acc: 32%] [G loss: 1.480468]\n",
      "[Epoch 0/50] [Batch 322/938] [D loss: 1.404944, acc: 35%] [G loss: 1.495043]\n",
      "[Epoch 0/50] [Batch 323/938] [D loss: 1.395387, acc: 42%] [G loss: 1.478640]\n",
      "[Epoch 0/50] [Batch 324/938] [D loss: 1.392581, acc: 46%] [G loss: 1.484770]\n",
      "[Epoch 0/50] [Batch 325/938] [D loss: 1.415249, acc: 34%] [G loss: 1.493263]\n",
      "[Epoch 0/50] [Batch 326/938] [D loss: 1.388294, acc: 39%] [G loss: 1.484678]\n",
      "[Epoch 0/50] [Batch 327/938] [D loss: 1.346777, acc: 47%] [G loss: 1.456691]\n",
      "[Epoch 0/50] [Batch 328/938] [D loss: 1.378689, acc: 40%] [G loss: 1.457609]\n",
      "[Epoch 0/50] [Batch 329/938] [D loss: 1.361542, acc: 43%] [G loss: 1.456886]\n",
      "[Epoch 0/50] [Batch 330/938] [D loss: 1.385977, acc: 41%] [G loss: 1.483136]\n",
      "[Epoch 0/50] [Batch 331/938] [D loss: 1.346951, acc: 43%] [G loss: 1.435281]\n",
      "[Epoch 0/50] [Batch 332/938] [D loss: 1.358032, acc: 45%] [G loss: 1.439668]\n",
      "[Epoch 0/50] [Batch 333/938] [D loss: 1.334056, acc: 50%] [G loss: 1.414364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/50] [Batch 334/938] [D loss: 1.379000, acc: 47%] [G loss: 1.418265]\n",
      "[Epoch 0/50] [Batch 335/938] [D loss: 1.396970, acc: 38%] [G loss: 1.449878]\n",
      "[Epoch 0/50] [Batch 336/938] [D loss: 1.382226, acc: 45%] [G loss: 1.421604]\n",
      "[Epoch 0/50] [Batch 337/938] [D loss: 1.393972, acc: 38%] [G loss: 1.491690]\n",
      "[Epoch 0/50] [Batch 338/938] [D loss: 1.388107, acc: 39%] [G loss: 1.458247]\n",
      "[Epoch 0/50] [Batch 339/938] [D loss: 1.372398, acc: 47%] [G loss: 1.430389]\n",
      "[Epoch 0/50] [Batch 340/938] [D loss: 1.373102, acc: 42%] [G loss: 1.432564]\n",
      "[Epoch 0/50] [Batch 341/938] [D loss: 1.375321, acc: 44%] [G loss: 1.462382]\n",
      "[Epoch 0/50] [Batch 342/938] [D loss: 1.363744, acc: 45%] [G loss: 1.492967]\n",
      "[Epoch 0/50] [Batch 343/938] [D loss: 1.353146, acc: 46%] [G loss: 1.452021]\n",
      "[Epoch 0/50] [Batch 344/938] [D loss: 1.370804, acc: 45%] [G loss: 1.446389]\n",
      "[Epoch 0/50] [Batch 345/938] [D loss: 1.341062, acc: 50%] [G loss: 1.466136]\n",
      "[Epoch 0/50] [Batch 346/938] [D loss: 1.377402, acc: 43%] [G loss: 1.494237]\n",
      "[Epoch 0/50] [Batch 347/938] [D loss: 1.381567, acc: 40%] [G loss: 1.495486]\n",
      "[Epoch 0/50] [Batch 348/938] [D loss: 1.391860, acc: 39%] [G loss: 1.537192]\n",
      "[Epoch 0/50] [Batch 349/938] [D loss: 1.346080, acc: 46%] [G loss: 1.477554]\n",
      "[Epoch 0/50] [Batch 350/938] [D loss: 1.378186, acc: 41%] [G loss: 1.475698]\n",
      "[Epoch 0/50] [Batch 351/938] [D loss: 1.404491, acc: 40%] [G loss: 1.512230]\n",
      "[Epoch 0/50] [Batch 352/938] [D loss: 1.381862, acc: 38%] [G loss: 1.494624]\n",
      "[Epoch 0/50] [Batch 353/938] [D loss: 1.360120, acc: 45%] [G loss: 1.452364]\n",
      "[Epoch 0/50] [Batch 354/938] [D loss: 1.354974, acc: 48%] [G loss: 1.516124]\n",
      "[Epoch 0/50] [Batch 355/938] [D loss: 1.346084, acc: 47%] [G loss: 1.499886]\n",
      "[Epoch 0/50] [Batch 356/938] [D loss: 1.348396, acc: 49%] [G loss: 1.464451]\n",
      "[Epoch 0/50] [Batch 357/938] [D loss: 1.361088, acc: 43%] [G loss: 1.466547]\n",
      "[Epoch 0/50] [Batch 358/938] [D loss: 1.351161, acc: 48%] [G loss: 1.457348]\n",
      "[Epoch 0/50] [Batch 359/938] [D loss: 1.344984, acc: 49%] [G loss: 1.463102]\n",
      "[Epoch 0/50] [Batch 360/938] [D loss: 1.356988, acc: 46%] [G loss: 1.456855]\n",
      "[Epoch 0/50] [Batch 361/938] [D loss: 1.322367, acc: 55%] [G loss: 1.424004]\n",
      "[Epoch 0/50] [Batch 362/938] [D loss: 1.340231, acc: 44%] [G loss: 1.480261]\n",
      "[Epoch 0/50] [Batch 363/938] [D loss: 1.321545, acc: 49%] [G loss: 1.469826]\n",
      "[Epoch 0/50] [Batch 364/938] [D loss: 1.342870, acc: 44%] [G loss: 1.425163]\n",
      "[Epoch 0/50] [Batch 365/938] [D loss: 1.359787, acc: 46%] [G loss: 1.454454]\n",
      "[Epoch 0/50] [Batch 366/938] [D loss: 1.364807, acc: 43%] [G loss: 1.464102]\n",
      "[Epoch 0/50] [Batch 367/938] [D loss: 1.353459, acc: 49%] [G loss: 1.445060]\n",
      "[Epoch 0/50] [Batch 368/938] [D loss: 1.358937, acc: 48%] [G loss: 1.445437]\n",
      "[Epoch 0/50] [Batch 369/938] [D loss: 1.324350, acc: 53%] [G loss: 1.436485]\n",
      "[Epoch 0/50] [Batch 370/938] [D loss: 1.326856, acc: 53%] [G loss: 1.437361]\n",
      "[Epoch 0/50] [Batch 371/938] [D loss: 1.374510, acc: 44%] [G loss: 1.446070]\n",
      "[Epoch 0/50] [Batch 372/938] [D loss: 1.353047, acc: 49%] [G loss: 1.408849]\n",
      "[Epoch 0/50] [Batch 373/938] [D loss: 1.335231, acc: 50%] [G loss: 1.429659]\n",
      "[Epoch 0/50] [Batch 374/938] [D loss: 1.333547, acc: 47%] [G loss: 1.461013]\n",
      "[Epoch 0/50] [Batch 375/938] [D loss: 1.348391, acc: 46%] [G loss: 1.428545]\n",
      "[Epoch 0/50] [Batch 376/938] [D loss: 1.333969, acc: 49%] [G loss: 1.457796]\n",
      "[Epoch 0/50] [Batch 377/938] [D loss: 1.328991, acc: 54%] [G loss: 1.464694]\n",
      "[Epoch 0/50] [Batch 378/938] [D loss: 1.346869, acc: 44%] [G loss: 1.488942]\n",
      "[Epoch 0/50] [Batch 379/938] [D loss: 1.353603, acc: 42%] [G loss: 1.477441]\n",
      "[Epoch 0/50] [Batch 380/938] [D loss: 1.323795, acc: 50%] [G loss: 1.436067]\n",
      "[Epoch 0/50] [Batch 381/938] [D loss: 1.316901, acc: 57%] [G loss: 1.422618]\n",
      "[Epoch 0/50] [Batch 382/938] [D loss: 1.361016, acc: 46%] [G loss: 1.490970]\n",
      "[Epoch 0/50] [Batch 383/938] [D loss: 1.322563, acc: 52%] [G loss: 1.488103]\n",
      "[Epoch 0/50] [Batch 384/938] [D loss: 1.311311, acc: 53%] [G loss: 1.419281]\n",
      "[Epoch 0/50] [Batch 385/938] [D loss: 1.312600, acc: 56%] [G loss: 1.434558]\n",
      "[Epoch 0/50] [Batch 386/938] [D loss: 1.319102, acc: 50%] [G loss: 1.436221]\n",
      "[Epoch 0/50] [Batch 387/938] [D loss: 1.346049, acc: 44%] [G loss: 1.475196]\n",
      "[Epoch 0/50] [Batch 388/938] [D loss: 1.348531, acc: 44%] [G loss: 1.487221]\n",
      "[Epoch 0/50] [Batch 389/938] [D loss: 1.333485, acc: 55%] [G loss: 1.432808]\n",
      "[Epoch 0/50] [Batch 390/938] [D loss: 1.330454, acc: 50%] [G loss: 1.429634]\n",
      "[Epoch 0/50] [Batch 391/938] [D loss: 1.350714, acc: 45%] [G loss: 1.439058]\n",
      "[Epoch 0/50] [Batch 392/938] [D loss: 1.335935, acc: 52%] [G loss: 1.450771]\n",
      "[Epoch 0/50] [Batch 393/938] [D loss: 1.319938, acc: 54%] [G loss: 1.415807]\n",
      "[Epoch 0/50] [Batch 394/938] [D loss: 1.321397, acc: 57%] [G loss: 1.421489]\n",
      "[Epoch 0/50] [Batch 395/938] [D loss: 1.332421, acc: 47%] [G loss: 1.408245]\n",
      "[Epoch 0/50] [Batch 396/938] [D loss: 1.289781, acc: 57%] [G loss: 1.394829]\n",
      "[Epoch 0/50] [Batch 397/938] [D loss: 1.309087, acc: 56%] [G loss: 1.403017]\n",
      "[Epoch 0/50] [Batch 398/938] [D loss: 1.313782, acc: 52%] [G loss: 1.431200]\n",
      "[Epoch 0/50] [Batch 399/938] [D loss: 1.289958, acc: 60%] [G loss: 1.377443]\n",
      "[Epoch 0/50] [Batch 400/938] [D loss: 1.331454, acc: 50%] [G loss: 1.423650]\n",
      "[Epoch 0/50] [Batch 401/938] [D loss: 1.330728, acc: 52%] [G loss: 1.442466]\n",
      "[Epoch 0/50] [Batch 402/938] [D loss: 1.311344, acc: 54%] [G loss: 1.436637]\n",
      "[Epoch 0/50] [Batch 403/938] [D loss: 1.320222, acc: 54%] [G loss: 1.435703]\n",
      "[Epoch 0/50] [Batch 404/938] [D loss: 1.323489, acc: 55%] [G loss: 1.433303]\n",
      "[Epoch 0/50] [Batch 405/938] [D loss: 1.319675, acc: 55%] [G loss: 1.430087]\n",
      "[Epoch 0/50] [Batch 406/938] [D loss: 1.335039, acc: 52%] [G loss: 1.438411]\n",
      "[Epoch 0/50] [Batch 407/938] [D loss: 1.335593, acc: 53%] [G loss: 1.415679]\n",
      "[Epoch 0/50] [Batch 408/938] [D loss: 1.319024, acc: 55%] [G loss: 1.415481]\n",
      "[Epoch 0/50] [Batch 409/938] [D loss: 1.310895, acc: 54%] [G loss: 1.417650]\n",
      "[Epoch 0/50] [Batch 410/938] [D loss: 1.294055, acc: 57%] [G loss: 1.398948]\n",
      "[Epoch 0/50] [Batch 411/938] [D loss: 1.343050, acc: 51%] [G loss: 1.434465]\n",
      "[Epoch 0/50] [Batch 412/938] [D loss: 1.279233, acc: 61%] [G loss: 1.408496]\n",
      "[Epoch 0/50] [Batch 413/938] [D loss: 1.327469, acc: 50%] [G loss: 1.403600]\n",
      "[Epoch 0/50] [Batch 414/938] [D loss: 1.302557, acc: 56%] [G loss: 1.428105]\n",
      "[Epoch 0/50] [Batch 415/938] [D loss: 1.307599, acc: 57%] [G loss: 1.410297]\n",
      "[Epoch 0/50] [Batch 416/938] [D loss: 1.301851, acc: 58%] [G loss: 1.393389]\n",
      "[Epoch 0/50] [Batch 417/938] [D loss: 1.305388, acc: 54%] [G loss: 1.419244]\n",
      "[Epoch 0/50] [Batch 418/938] [D loss: 1.290603, acc: 59%] [G loss: 1.428341]\n",
      "[Epoch 0/50] [Batch 419/938] [D loss: 1.321393, acc: 53%] [G loss: 1.429631]\n",
      "[Epoch 0/50] [Batch 420/938] [D loss: 1.276454, acc: 62%] [G loss: 1.384042]\n",
      "[Epoch 0/50] [Batch 421/938] [D loss: 1.303908, acc: 54%] [G loss: 1.429947]\n",
      "[Epoch 0/50] [Batch 422/938] [D loss: 1.298221, acc: 53%] [G loss: 1.419829]\n",
      "[Epoch 0/50] [Batch 423/938] [D loss: 1.277684, acc: 60%] [G loss: 1.353664]\n",
      "[Epoch 0/50] [Batch 424/938] [D loss: 1.320415, acc: 51%] [G loss: 1.442381]\n",
      "[Epoch 0/50] [Batch 425/938] [D loss: 1.318254, acc: 49%] [G loss: 1.471485]\n",
      "[Epoch 0/50] [Batch 426/938] [D loss: 1.293528, acc: 57%] [G loss: 1.448616]\n",
      "[Epoch 0/50] [Batch 427/938] [D loss: 1.300879, acc: 57%] [G loss: 1.406947]\n",
      "[Epoch 0/50] [Batch 428/938] [D loss: 1.275305, acc: 61%] [G loss: 1.369585]\n",
      "[Epoch 0/50] [Batch 429/938] [D loss: 1.311711, acc: 53%] [G loss: 1.434801]\n",
      "[Epoch 0/50] [Batch 430/938] [D loss: 1.293436, acc: 59%] [G loss: 1.409320]\n",
      "[Epoch 0/50] [Batch 431/938] [D loss: 1.317947, acc: 53%] [G loss: 1.449345]\n",
      "[Epoch 0/50] [Batch 432/938] [D loss: 1.276752, acc: 60%] [G loss: 1.412199]\n",
      "[Epoch 0/50] [Batch 433/938] [D loss: 1.279322, acc: 57%] [G loss: 1.366210]\n",
      "[Epoch 0/50] [Batch 434/938] [D loss: 1.306723, acc: 57%] [G loss: 1.351131]\n",
      "[Epoch 0/50] [Batch 435/938] [D loss: 1.271100, acc: 58%] [G loss: 1.372922]\n",
      "[Epoch 0/50] [Batch 436/938] [D loss: 1.317223, acc: 52%] [G loss: 1.400908]\n",
      "[Epoch 0/50] [Batch 437/938] [D loss: 1.321689, acc: 53%] [G loss: 1.409230]\n",
      "[Epoch 0/50] [Batch 438/938] [D loss: 1.303021, acc: 57%] [G loss: 1.365424]\n",
      "[Epoch 0/50] [Batch 439/938] [D loss: 1.285045, acc: 59%] [G loss: 1.390339]\n",
      "[Epoch 0/50] [Batch 440/938] [D loss: 1.254012, acc: 68%] [G loss: 1.347554]\n",
      "[Epoch 0/50] [Batch 441/938] [D loss: 1.279566, acc: 61%] [G loss: 1.402840]\n",
      "[Epoch 0/50] [Batch 442/938] [D loss: 1.284087, acc: 60%] [G loss: 1.420539]\n",
      "[Epoch 0/50] [Batch 443/938] [D loss: 1.265757, acc: 62%] [G loss: 1.402606]\n",
      "[Epoch 0/50] [Batch 444/938] [D loss: 1.275237, acc: 62%] [G loss: 1.373658]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/50] [Batch 445/938] [D loss: 1.318557, acc: 53%] [G loss: 1.422038]\n",
      "[Epoch 0/50] [Batch 446/938] [D loss: 1.289460, acc: 57%] [G loss: 1.414374]\n",
      "[Epoch 0/50] [Batch 447/938] [D loss: 1.299673, acc: 58%] [G loss: 1.374526]\n",
      "[Epoch 0/50] [Batch 448/938] [D loss: 1.310941, acc: 55%] [G loss: 1.405298]\n",
      "[Epoch 0/50] [Batch 449/938] [D loss: 1.293378, acc: 59%] [G loss: 1.412966]\n",
      "[Epoch 0/50] [Batch 450/938] [D loss: 1.271735, acc: 61%] [G loss: 1.420752]\n",
      "[Epoch 0/50] [Batch 451/938] [D loss: 1.272011, acc: 57%] [G loss: 1.389796]\n",
      "[Epoch 0/50] [Batch 452/938] [D loss: 1.260887, acc: 65%] [G loss: 1.336004]\n",
      "[Epoch 0/50] [Batch 453/938] [D loss: 1.307809, acc: 55%] [G loss: 1.413485]\n",
      "[Epoch 0/50] [Batch 454/938] [D loss: 1.305077, acc: 57%] [G loss: 1.391756]\n",
      "[Epoch 0/50] [Batch 455/938] [D loss: 1.264672, acc: 61%] [G loss: 1.339488]\n",
      "[Epoch 0/50] [Batch 456/938] [D loss: 1.279811, acc: 60%] [G loss: 1.418109]\n",
      "[Epoch 0/50] [Batch 457/938] [D loss: 1.293202, acc: 59%] [G loss: 1.376208]\n",
      "[Epoch 0/50] [Batch 458/938] [D loss: 1.269517, acc: 68%] [G loss: 1.339063]\n",
      "[Epoch 0/50] [Batch 459/938] [D loss: 1.302297, acc: 53%] [G loss: 1.389281]\n",
      "[Epoch 0/50] [Batch 460/938] [D loss: 1.261674, acc: 64%] [G loss: 1.351434]\n",
      "[Epoch 0/50] [Batch 461/938] [D loss: 1.263987, acc: 67%] [G loss: 1.322818]\n",
      "[Epoch 0/50] [Batch 462/938] [D loss: 1.275309, acc: 59%] [G loss: 1.433471]\n",
      "[Epoch 0/50] [Batch 463/938] [D loss: 1.288968, acc: 58%] [G loss: 1.399504]\n",
      "[Epoch 0/50] [Batch 464/938] [D loss: 1.331564, acc: 50%] [G loss: 1.368672]\n",
      "[Epoch 0/50] [Batch 465/938] [D loss: 1.263149, acc: 65%] [G loss: 1.347841]\n",
      "[Epoch 0/50] [Batch 466/938] [D loss: 1.266493, acc: 65%] [G loss: 1.413991]\n",
      "[Epoch 0/50] [Batch 467/938] [D loss: 1.304150, acc: 60%] [G loss: 1.386796]\n",
      "[Epoch 0/50] [Batch 468/938] [D loss: 1.265702, acc: 60%] [G loss: 1.406378]\n",
      "[Epoch 0/50] [Batch 469/938] [D loss: 1.280780, acc: 60%] [G loss: 1.402755]\n",
      "[Epoch 0/50] [Batch 470/938] [D loss: 1.261186, acc: 63%] [G loss: 1.376907]\n",
      "[Epoch 0/50] [Batch 471/938] [D loss: 1.272161, acc: 62%] [G loss: 1.383574]\n",
      "[Epoch 0/50] [Batch 472/938] [D loss: 1.260441, acc: 64%] [G loss: 1.313454]\n",
      "[Epoch 0/50] [Batch 473/938] [D loss: 1.288291, acc: 58%] [G loss: 1.399829]\n",
      "[Epoch 0/50] [Batch 474/938] [D loss: 1.262591, acc: 60%] [G loss: 1.365537]\n",
      "[Epoch 0/50] [Batch 475/938] [D loss: 1.291619, acc: 55%] [G loss: 1.374112]\n",
      "[Epoch 0/50] [Batch 476/938] [D loss: 1.282579, acc: 58%] [G loss: 1.373048]\n",
      "[Epoch 0/50] [Batch 477/938] [D loss: 1.253374, acc: 60%] [G loss: 1.327333]\n",
      "[Epoch 0/50] [Batch 478/938] [D loss: 1.239494, acc: 66%] [G loss: 1.346226]\n",
      "[Epoch 0/50] [Batch 479/938] [D loss: 1.272251, acc: 60%] [G loss: 1.365744]\n",
      "[Epoch 0/50] [Batch 480/938] [D loss: 1.308959, acc: 57%] [G loss: 1.383914]\n",
      "[Epoch 0/50] [Batch 481/938] [D loss: 1.244673, acc: 66%] [G loss: 1.361372]\n",
      "[Epoch 0/50] [Batch 482/938] [D loss: 1.285869, acc: 55%] [G loss: 1.451182]\n",
      "[Epoch 0/50] [Batch 483/938] [D loss: 1.265741, acc: 65%] [G loss: 1.381202]\n",
      "[Epoch 0/50] [Batch 484/938] [D loss: 1.269512, acc: 62%] [G loss: 1.382706]\n",
      "[Epoch 0/50] [Batch 485/938] [D loss: 1.271661, acc: 61%] [G loss: 1.360968]\n",
      "[Epoch 0/50] [Batch 486/938] [D loss: 1.283439, acc: 59%] [G loss: 1.410685]\n",
      "[Epoch 0/50] [Batch 487/938] [D loss: 1.231271, acc: 71%] [G loss: 1.280938]\n",
      "[Epoch 0/50] [Batch 488/938] [D loss: 1.245659, acc: 69%] [G loss: 1.336363]\n",
      "[Epoch 0/50] [Batch 489/938] [D loss: 1.262474, acc: 61%] [G loss: 1.369517]\n",
      "[Epoch 0/50] [Batch 490/938] [D loss: 1.273692, acc: 64%] [G loss: 1.367643]\n",
      "[Epoch 0/50] [Batch 491/938] [D loss: 1.276819, acc: 60%] [G loss: 1.385412]\n",
      "[Epoch 0/50] [Batch 492/938] [D loss: 1.255830, acc: 65%] [G loss: 1.395692]\n",
      "[Epoch 0/50] [Batch 493/938] [D loss: 1.247796, acc: 64%] [G loss: 1.378360]\n",
      "[Epoch 0/50] [Batch 494/938] [D loss: 1.267205, acc: 63%] [G loss: 1.377199]\n",
      "[Epoch 0/50] [Batch 495/938] [D loss: 1.266164, acc: 60%] [G loss: 1.399973]\n",
      "[Epoch 0/50] [Batch 496/938] [D loss: 1.285018, acc: 58%] [G loss: 1.368574]\n",
      "[Epoch 0/50] [Batch 497/938] [D loss: 1.263458, acc: 60%] [G loss: 1.347587]\n",
      "[Epoch 0/50] [Batch 498/938] [D loss: 1.265809, acc: 63%] [G loss: 1.343756]\n",
      "[Epoch 0/50] [Batch 499/938] [D loss: 1.272534, acc: 57%] [G loss: 1.396437]\n",
      "[Epoch 0/50] [Batch 500/938] [D loss: 1.265798, acc: 62%] [G loss: 1.377179]\n",
      "[Epoch 0/50] [Batch 501/938] [D loss: 1.280518, acc: 59%] [G loss: 1.378860]\n",
      "[Epoch 0/50] [Batch 502/938] [D loss: 1.256543, acc: 60%] [G loss: 1.367446]\n",
      "[Epoch 0/50] [Batch 503/938] [D loss: 1.256229, acc: 64%] [G loss: 1.356006]\n",
      "[Epoch 0/50] [Batch 504/938] [D loss: 1.283579, acc: 59%] [G loss: 1.404115]\n",
      "[Epoch 0/50] [Batch 505/938] [D loss: 1.277109, acc: 61%] [G loss: 1.378049]\n",
      "[Epoch 0/50] [Batch 506/938] [D loss: 1.281504, acc: 59%] [G loss: 1.363702]\n",
      "[Epoch 0/50] [Batch 507/938] [D loss: 1.252365, acc: 67%] [G loss: 1.354486]\n",
      "[Epoch 0/50] [Batch 508/938] [D loss: 1.254554, acc: 64%] [G loss: 1.349676]\n",
      "[Epoch 0/50] [Batch 509/938] [D loss: 1.269582, acc: 64%] [G loss: 1.339822]\n",
      "[Epoch 0/50] [Batch 510/938] [D loss: 1.255444, acc: 62%] [G loss: 1.309581]\n",
      "[Epoch 0/50] [Batch 511/938] [D loss: 1.242678, acc: 68%] [G loss: 1.298375]\n",
      "[Epoch 0/50] [Batch 512/938] [D loss: 1.244299, acc: 65%] [G loss: 1.324940]\n",
      "[Epoch 0/50] [Batch 513/938] [D loss: 1.272415, acc: 58%] [G loss: 1.338918]\n",
      "[Epoch 0/50] [Batch 514/938] [D loss: 1.259896, acc: 64%] [G loss: 1.329100]\n",
      "[Epoch 0/50] [Batch 515/938] [D loss: 1.249679, acc: 67%] [G loss: 1.335767]\n",
      "[Epoch 0/50] [Batch 516/938] [D loss: 1.277191, acc: 61%] [G loss: 1.398374]\n",
      "[Epoch 0/50] [Batch 517/938] [D loss: 1.220441, acc: 71%] [G loss: 1.301709]\n",
      "[Epoch 0/50] [Batch 518/938] [D loss: 1.233834, acc: 65%] [G loss: 1.351419]\n",
      "[Epoch 0/50] [Batch 519/938] [D loss: 1.243160, acc: 63%] [G loss: 1.362770]\n",
      "[Epoch 0/50] [Batch 520/938] [D loss: 1.234240, acc: 72%] [G loss: 1.338842]\n",
      "[Epoch 0/50] [Batch 521/938] [D loss: 1.230808, acc: 67%] [G loss: 1.363016]\n",
      "[Epoch 0/50] [Batch 522/938] [D loss: 1.222662, acc: 71%] [G loss: 1.314051]\n",
      "[Epoch 0/50] [Batch 523/938] [D loss: 1.254990, acc: 64%] [G loss: 1.354818]\n",
      "[Epoch 0/50] [Batch 524/938] [D loss: 1.234483, acc: 65%] [G loss: 1.405530]\n",
      "[Epoch 0/50] [Batch 525/938] [D loss: 1.256768, acc: 65%] [G loss: 1.355350]\n",
      "[Epoch 0/50] [Batch 526/938] [D loss: 1.221064, acc: 70%] [G loss: 1.351806]\n",
      "[Epoch 0/50] [Batch 527/938] [D loss: 1.206200, acc: 70%] [G loss: 1.347643]\n",
      "[Epoch 0/50] [Batch 528/938] [D loss: 1.219584, acc: 71%] [G loss: 1.302176]\n",
      "[Epoch 0/50] [Batch 529/938] [D loss: 1.229220, acc: 67%] [G loss: 1.327926]\n",
      "[Epoch 0/50] [Batch 530/938] [D loss: 1.243736, acc: 65%] [G loss: 1.343430]\n",
      "[Epoch 0/50] [Batch 531/938] [D loss: 1.220258, acc: 71%] [G loss: 1.328198]\n",
      "[Epoch 0/50] [Batch 532/938] [D loss: 1.265375, acc: 62%] [G loss: 1.391138]\n",
      "[Epoch 0/50] [Batch 533/938] [D loss: 1.219256, acc: 69%] [G loss: 1.316433]\n",
      "[Epoch 0/50] [Batch 534/938] [D loss: 1.245776, acc: 64%] [G loss: 1.332963]\n",
      "[Epoch 0/50] [Batch 535/938] [D loss: 1.220227, acc: 69%] [G loss: 1.331901]\n",
      "[Epoch 0/50] [Batch 536/938] [D loss: 1.191497, acc: 74%] [G loss: 1.319732]\n",
      "[Epoch 0/50] [Batch 537/938] [D loss: 1.206203, acc: 72%] [G loss: 1.305562]\n",
      "[Epoch 0/50] [Batch 538/938] [D loss: 1.202803, acc: 70%] [G loss: 1.367418]\n",
      "[Epoch 0/50] [Batch 539/938] [D loss: 1.237640, acc: 67%] [G loss: 1.378380]\n",
      "[Epoch 0/50] [Batch 540/938] [D loss: 1.245841, acc: 69%] [G loss: 1.363458]\n",
      "[Epoch 0/50] [Batch 541/938] [D loss: 1.225718, acc: 74%] [G loss: 1.347607]\n",
      "[Epoch 0/50] [Batch 542/938] [D loss: 1.218386, acc: 70%] [G loss: 1.336007]\n",
      "[Epoch 0/50] [Batch 543/938] [D loss: 1.219695, acc: 70%] [G loss: 1.308733]\n",
      "[Epoch 0/50] [Batch 544/938] [D loss: 1.205993, acc: 71%] [G loss: 1.328588]\n",
      "[Epoch 0/50] [Batch 545/938] [D loss: 1.255389, acc: 61%] [G loss: 1.358593]\n",
      "[Epoch 0/50] [Batch 546/938] [D loss: 1.218985, acc: 74%] [G loss: 1.325844]\n",
      "[Epoch 0/50] [Batch 547/938] [D loss: 1.215235, acc: 74%] [G loss: 1.344512]\n",
      "[Epoch 0/50] [Batch 548/938] [D loss: 1.188716, acc: 74%] [G loss: 1.313292]\n",
      "[Epoch 0/50] [Batch 549/938] [D loss: 1.232613, acc: 67%] [G loss: 1.330282]\n",
      "[Epoch 0/50] [Batch 550/938] [D loss: 1.244438, acc: 64%] [G loss: 1.349890]\n",
      "[Epoch 0/50] [Batch 551/938] [D loss: 1.224620, acc: 72%] [G loss: 1.309971]\n",
      "[Epoch 0/50] [Batch 552/938] [D loss: 1.234374, acc: 62%] [G loss: 1.352004]\n",
      "[Epoch 0/50] [Batch 553/938] [D loss: 1.242010, acc: 68%] [G loss: 1.318729]\n",
      "[Epoch 0/50] [Batch 554/938] [D loss: 1.229058, acc: 70%] [G loss: 1.313940]\n",
      "[Epoch 0/50] [Batch 555/938] [D loss: 1.207184, acc: 71%] [G loss: 1.290106]\n",
      "[Epoch 0/50] [Batch 556/938] [D loss: 1.201158, acc: 71%] [G loss: 1.324487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/50] [Batch 557/938] [D loss: 1.249811, acc: 63%] [G loss: 1.368356]\n",
      "[Epoch 0/50] [Batch 558/938] [D loss: 1.210265, acc: 72%] [G loss: 1.318721]\n",
      "[Epoch 0/50] [Batch 559/938] [D loss: 1.247462, acc: 67%] [G loss: 1.286583]\n",
      "[Epoch 0/50] [Batch 560/938] [D loss: 1.235542, acc: 67%] [G loss: 1.347811]\n",
      "[Epoch 0/50] [Batch 561/938] [D loss: 1.253295, acc: 64%] [G loss: 1.341233]\n",
      "[Epoch 0/50] [Batch 562/938] [D loss: 1.215436, acc: 71%] [G loss: 1.291972]\n",
      "[Epoch 0/50] [Batch 563/938] [D loss: 1.212409, acc: 69%] [G loss: 1.357070]\n",
      "[Epoch 0/50] [Batch 564/938] [D loss: 1.205850, acc: 75%] [G loss: 1.281514]\n",
      "[Epoch 0/50] [Batch 565/938] [D loss: 1.205143, acc: 73%] [G loss: 1.322948]\n",
      "[Epoch 0/50] [Batch 566/938] [D loss: 1.201415, acc: 71%] [G loss: 1.314462]\n",
      "[Epoch 0/50] [Batch 567/938] [D loss: 1.206472, acc: 71%] [G loss: 1.339463]\n",
      "[Epoch 0/50] [Batch 568/938] [D loss: 1.231106, acc: 66%] [G loss: 1.333269]\n",
      "[Epoch 0/50] [Batch 569/938] [D loss: 1.208922, acc: 70%] [G loss: 1.309179]\n",
      "[Epoch 0/50] [Batch 570/938] [D loss: 1.213094, acc: 74%] [G loss: 1.283295]\n",
      "[Epoch 0/50] [Batch 571/938] [D loss: 1.222537, acc: 66%] [G loss: 1.313415]\n",
      "[Epoch 0/50] [Batch 572/938] [D loss: 1.194378, acc: 74%] [G loss: 1.306338]\n",
      "[Epoch 0/50] [Batch 573/938] [D loss: 1.211566, acc: 73%] [G loss: 1.326333]\n",
      "[Epoch 0/50] [Batch 574/938] [D loss: 1.211153, acc: 74%] [G loss: 1.277632]\n",
      "[Epoch 0/50] [Batch 575/938] [D loss: 1.229161, acc: 64%] [G loss: 1.363313]\n",
      "[Epoch 0/50] [Batch 576/938] [D loss: 1.218637, acc: 66%] [G loss: 1.296045]\n",
      "[Epoch 0/50] [Batch 577/938] [D loss: 1.198025, acc: 75%] [G loss: 1.323623]\n",
      "[Epoch 0/50] [Batch 578/938] [D loss: 1.208799, acc: 67%] [G loss: 1.349973]\n",
      "[Epoch 0/50] [Batch 579/938] [D loss: 1.228341, acc: 64%] [G loss: 1.402561]\n",
      "[Epoch 0/50] [Batch 580/938] [D loss: 1.191666, acc: 75%] [G loss: 1.278536]\n",
      "[Epoch 0/50] [Batch 581/938] [D loss: 1.238229, acc: 66%] [G loss: 1.316294]\n",
      "[Epoch 0/50] [Batch 582/938] [D loss: 1.218103, acc: 66%] [G loss: 1.325900]\n",
      "[Epoch 0/50] [Batch 583/938] [D loss: 1.232721, acc: 67%] [G loss: 1.285034]\n",
      "[Epoch 0/50] [Batch 584/938] [D loss: 1.225362, acc: 69%] [G loss: 1.333187]\n",
      "[Epoch 0/50] [Batch 585/938] [D loss: 1.199263, acc: 71%] [G loss: 1.269009]\n",
      "[Epoch 0/50] [Batch 586/938] [D loss: 1.219036, acc: 71%] [G loss: 1.299525]\n",
      "[Epoch 0/50] [Batch 587/938] [D loss: 1.200758, acc: 74%] [G loss: 1.314753]\n",
      "[Epoch 0/50] [Batch 588/938] [D loss: 1.188287, acc: 76%] [G loss: 1.317366]\n",
      "[Epoch 0/50] [Batch 589/938] [D loss: 1.211617, acc: 70%] [G loss: 1.316114]\n",
      "[Epoch 0/50] [Batch 590/938] [D loss: 1.219499, acc: 68%] [G loss: 1.334253]\n",
      "[Epoch 0/50] [Batch 591/938] [D loss: 1.211509, acc: 69%] [G loss: 1.320451]\n",
      "[Epoch 0/50] [Batch 592/938] [D loss: 1.189576, acc: 80%] [G loss: 1.297142]\n",
      "[Epoch 0/50] [Batch 593/938] [D loss: 1.203387, acc: 69%] [G loss: 1.349210]\n",
      "[Epoch 0/50] [Batch 594/938] [D loss: 1.150965, acc: 82%] [G loss: 1.227672]\n",
      "[Epoch 0/50] [Batch 595/938] [D loss: 1.167356, acc: 81%] [G loss: 1.277919]\n",
      "[Epoch 0/50] [Batch 596/938] [D loss: 1.204413, acc: 72%] [G loss: 1.330053]\n",
      "[Epoch 0/50] [Batch 597/938] [D loss: 1.212742, acc: 69%] [G loss: 1.315751]\n",
      "[Epoch 0/50] [Batch 598/938] [D loss: 1.205793, acc: 72%] [G loss: 1.286013]\n",
      "[Epoch 0/50] [Batch 599/938] [D loss: 1.194215, acc: 72%] [G loss: 1.319724]\n",
      "[Epoch 0/50] [Batch 600/938] [D loss: 1.198650, acc: 76%] [G loss: 1.249589]\n",
      "[Epoch 0/50] [Batch 601/938] [D loss: 1.203589, acc: 72%] [G loss: 1.296771]\n",
      "[Epoch 0/50] [Batch 602/938] [D loss: 1.180623, acc: 74%] [G loss: 1.317396]\n",
      "[Epoch 0/50] [Batch 603/938] [D loss: 1.184392, acc: 75%] [G loss: 1.317687]\n",
      "[Epoch 0/50] [Batch 604/938] [D loss: 1.193530, acc: 72%] [G loss: 1.352319]\n",
      "[Epoch 0/50] [Batch 605/938] [D loss: 1.176620, acc: 78%] [G loss: 1.268294]\n",
      "[Epoch 0/50] [Batch 606/938] [D loss: 1.175647, acc: 77%] [G loss: 1.334070]\n",
      "[Epoch 0/50] [Batch 607/938] [D loss: 1.180761, acc: 76%] [G loss: 1.263531]\n",
      "[Epoch 0/50] [Batch 608/938] [D loss: 1.211577, acc: 71%] [G loss: 1.311074]\n",
      "[Epoch 0/50] [Batch 609/938] [D loss: 1.187163, acc: 78%] [G loss: 1.280270]\n",
      "[Epoch 0/50] [Batch 610/938] [D loss: 1.188091, acc: 73%] [G loss: 1.288359]\n",
      "[Epoch 0/50] [Batch 611/938] [D loss: 1.183119, acc: 76%] [G loss: 1.277307]\n",
      "[Epoch 0/50] [Batch 612/938] [D loss: 1.191802, acc: 75%] [G loss: 1.311193]\n",
      "[Epoch 0/50] [Batch 613/938] [D loss: 1.188138, acc: 72%] [G loss: 1.319468]\n",
      "[Epoch 0/50] [Batch 614/938] [D loss: 1.202696, acc: 73%] [G loss: 1.308824]\n",
      "[Epoch 0/50] [Batch 615/938] [D loss: 1.188414, acc: 77%] [G loss: 1.245289]\n",
      "[Epoch 0/50] [Batch 616/938] [D loss: 1.163961, acc: 78%] [G loss: 1.254340]\n",
      "[Epoch 0/50] [Batch 617/938] [D loss: 1.191331, acc: 73%] [G loss: 1.328647]\n",
      "[Epoch 0/50] [Batch 618/938] [D loss: 1.186481, acc: 76%] [G loss: 1.224287]\n",
      "[Epoch 0/50] [Batch 619/938] [D loss: 1.170847, acc: 79%] [G loss: 1.258376]\n",
      "[Epoch 0/50] [Batch 620/938] [D loss: 1.192505, acc: 75%] [G loss: 1.281108]\n",
      "[Epoch 0/50] [Batch 621/938] [D loss: 1.189480, acc: 78%] [G loss: 1.236476]\n",
      "[Epoch 0/50] [Batch 622/938] [D loss: 1.172947, acc: 76%] [G loss: 1.308312]\n",
      "[Epoch 0/50] [Batch 623/938] [D loss: 1.182091, acc: 78%] [G loss: 1.305801]\n",
      "[Epoch 0/50] [Batch 624/938] [D loss: 1.171597, acc: 75%] [G loss: 1.274727]\n",
      "[Epoch 0/50] [Batch 625/938] [D loss: 1.189117, acc: 72%] [G loss: 1.251945]\n",
      "[Epoch 0/50] [Batch 626/938] [D loss: 1.162484, acc: 80%] [G loss: 1.245762]\n",
      "[Epoch 0/50] [Batch 627/938] [D loss: 1.146876, acc: 83%] [G loss: 1.240019]\n",
      "[Epoch 0/50] [Batch 628/938] [D loss: 1.167117, acc: 81%] [G loss: 1.272379]\n",
      "[Epoch 0/50] [Batch 629/938] [D loss: 1.185604, acc: 79%] [G loss: 1.243052]\n",
      "[Epoch 0/50] [Batch 630/938] [D loss: 1.187874, acc: 77%] [G loss: 1.316872]\n",
      "[Epoch 0/50] [Batch 631/938] [D loss: 1.188640, acc: 73%] [G loss: 1.261951]\n",
      "[Epoch 0/50] [Batch 632/938] [D loss: 1.160015, acc: 80%] [G loss: 1.266907]\n",
      "[Epoch 0/50] [Batch 633/938] [D loss: 1.208579, acc: 68%] [G loss: 1.363156]\n",
      "[Epoch 0/50] [Batch 634/938] [D loss: 1.189916, acc: 75%] [G loss: 1.309294]\n",
      "[Epoch 0/50] [Batch 635/938] [D loss: 1.174493, acc: 78%] [G loss: 1.257559]\n",
      "[Epoch 0/50] [Batch 636/938] [D loss: 1.163058, acc: 77%] [G loss: 1.251480]\n",
      "[Epoch 0/50] [Batch 637/938] [D loss: 1.201272, acc: 70%] [G loss: 1.285921]\n",
      "[Epoch 0/50] [Batch 638/938] [D loss: 1.183687, acc: 72%] [G loss: 1.346467]\n",
      "[Epoch 0/50] [Batch 639/938] [D loss: 1.177988, acc: 77%] [G loss: 1.326931]\n",
      "[Epoch 0/50] [Batch 640/938] [D loss: 1.167329, acc: 82%] [G loss: 1.230796]\n",
      "[Epoch 0/50] [Batch 641/938] [D loss: 1.154428, acc: 77%] [G loss: 1.302918]\n",
      "[Epoch 0/50] [Batch 642/938] [D loss: 1.176881, acc: 76%] [G loss: 1.296707]\n",
      "[Epoch 0/50] [Batch 643/938] [D loss: 1.153681, acc: 81%] [G loss: 1.237197]\n",
      "[Epoch 0/50] [Batch 644/938] [D loss: 1.152679, acc: 74%] [G loss: 1.348455]\n",
      "[Epoch 0/50] [Batch 645/938] [D loss: 1.180030, acc: 78%] [G loss: 1.277066]\n",
      "[Epoch 0/50] [Batch 646/938] [D loss: 1.180758, acc: 76%] [G loss: 1.296282]\n",
      "[Epoch 0/50] [Batch 647/938] [D loss: 1.205612, acc: 71%] [G loss: 1.275068]\n",
      "[Epoch 0/50] [Batch 648/938] [D loss: 1.186452, acc: 72%] [G loss: 1.300591]\n",
      "[Epoch 0/50] [Batch 649/938] [D loss: 1.144755, acc: 82%] [G loss: 1.198398]\n",
      "[Epoch 0/50] [Batch 650/938] [D loss: 1.159197, acc: 77%] [G loss: 1.256477]\n",
      "[Epoch 0/50] [Batch 651/938] [D loss: 1.181481, acc: 78%] [G loss: 1.253149]\n",
      "[Epoch 0/50] [Batch 652/938] [D loss: 1.173151, acc: 75%] [G loss: 1.312721]\n",
      "[Epoch 0/50] [Batch 653/938] [D loss: 1.162092, acc: 75%] [G loss: 1.283170]\n",
      "[Epoch 0/50] [Batch 654/938] [D loss: 1.132273, acc: 85%] [G loss: 1.225023]\n",
      "[Epoch 0/50] [Batch 655/938] [D loss: 1.174541, acc: 75%] [G loss: 1.260152]\n",
      "[Epoch 0/50] [Batch 656/938] [D loss: 1.165421, acc: 80%] [G loss: 1.291450]\n",
      "[Epoch 0/50] [Batch 657/938] [D loss: 1.207643, acc: 75%] [G loss: 1.297932]\n",
      "[Epoch 0/50] [Batch 658/938] [D loss: 1.172236, acc: 75%] [G loss: 1.267506]\n",
      "[Epoch 0/50] [Batch 659/938] [D loss: 1.173579, acc: 76%] [G loss: 1.231701]\n",
      "[Epoch 0/50] [Batch 660/938] [D loss: 1.176988, acc: 71%] [G loss: 1.261856]\n",
      "[Epoch 0/50] [Batch 661/938] [D loss: 1.170042, acc: 79%] [G loss: 1.192726]\n",
      "[Epoch 0/50] [Batch 662/938] [D loss: 1.158353, acc: 79%] [G loss: 1.227412]\n",
      "[Epoch 0/50] [Batch 663/938] [D loss: 1.177099, acc: 75%] [G loss: 1.297807]\n",
      "[Epoch 0/50] [Batch 664/938] [D loss: 1.145505, acc: 79%] [G loss: 1.309178]\n",
      "[Epoch 0/50] [Batch 665/938] [D loss: 1.137161, acc: 83%] [G loss: 1.268436]\n",
      "[Epoch 0/50] [Batch 666/938] [D loss: 1.168373, acc: 77%] [G loss: 1.321622]\n",
      "[Epoch 0/50] [Batch 667/938] [D loss: 1.149969, acc: 82%] [G loss: 1.250181]\n",
      "[Epoch 0/50] [Batch 668/938] [D loss: 1.139331, acc: 82%] [G loss: 1.324508]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/50] [Batch 669/938] [D loss: 1.161547, acc: 76%] [G loss: 1.290624]\n",
      "[Epoch 0/50] [Batch 670/938] [D loss: 1.155250, acc: 81%] [G loss: 1.243828]\n",
      "[Epoch 0/50] [Batch 671/938] [D loss: 1.165529, acc: 79%] [G loss: 1.222974]\n",
      "[Epoch 0/50] [Batch 672/938] [D loss: 1.179815, acc: 71%] [G loss: 1.321985]\n",
      "[Epoch 0/50] [Batch 673/938] [D loss: 1.133043, acc: 79%] [G loss: 1.267390]\n",
      "[Epoch 0/50] [Batch 674/938] [D loss: 1.175322, acc: 74%] [G loss: 1.303156]\n",
      "[Epoch 0/50] [Batch 675/938] [D loss: 1.140501, acc: 85%] [G loss: 1.269597]\n",
      "[Epoch 0/50] [Batch 676/938] [D loss: 1.152860, acc: 76%] [G loss: 1.290836]\n",
      "[Epoch 0/50] [Batch 677/938] [D loss: 1.161430, acc: 77%] [G loss: 1.309044]\n",
      "[Epoch 0/50] [Batch 678/938] [D loss: 1.156308, acc: 82%] [G loss: 1.254215]\n",
      "[Epoch 0/50] [Batch 679/938] [D loss: 1.151240, acc: 81%] [G loss: 1.237357]\n",
      "[Epoch 0/50] [Batch 680/938] [D loss: 1.131635, acc: 83%] [G loss: 1.271891]\n",
      "[Epoch 0/50] [Batch 681/938] [D loss: 1.156160, acc: 78%] [G loss: 1.304822]\n",
      "[Epoch 0/50] [Batch 682/938] [D loss: 1.172984, acc: 79%] [G loss: 1.254395]\n",
      "[Epoch 0/50] [Batch 683/938] [D loss: 1.145154, acc: 82%] [G loss: 1.186834]\n",
      "[Epoch 0/50] [Batch 684/938] [D loss: 1.168168, acc: 77%] [G loss: 1.272380]\n",
      "[Epoch 0/50] [Batch 685/938] [D loss: 1.183884, acc: 76%] [G loss: 1.240672]\n",
      "[Epoch 0/50] [Batch 686/938] [D loss: 1.182076, acc: 72%] [G loss: 1.255466]\n",
      "[Epoch 0/50] [Batch 687/938] [D loss: 1.168468, acc: 78%] [G loss: 1.280694]\n",
      "[Epoch 0/50] [Batch 688/938] [D loss: 1.176990, acc: 79%] [G loss: 1.269767]\n",
      "[Epoch 0/50] [Batch 689/938] [D loss: 1.145595, acc: 81%] [G loss: 1.276994]\n",
      "[Epoch 0/50] [Batch 690/938] [D loss: 1.180082, acc: 75%] [G loss: 1.274748]\n",
      "[Epoch 0/50] [Batch 691/938] [D loss: 1.151542, acc: 79%] [G loss: 1.256859]\n",
      "[Epoch 0/50] [Batch 692/938] [D loss: 1.127060, acc: 82%] [G loss: 1.226715]\n",
      "[Epoch 0/50] [Batch 693/938] [D loss: 1.174040, acc: 78%] [G loss: 1.306732]\n",
      "[Epoch 0/50] [Batch 694/938] [D loss: 1.161392, acc: 80%] [G loss: 1.289212]\n",
      "[Epoch 0/50] [Batch 695/938] [D loss: 1.127657, acc: 85%] [G loss: 1.233328]\n",
      "[Epoch 0/50] [Batch 696/938] [D loss: 1.144041, acc: 76%] [G loss: 1.285741]\n",
      "[Epoch 0/50] [Batch 697/938] [D loss: 1.153986, acc: 78%] [G loss: 1.268794]\n",
      "[Epoch 0/50] [Batch 698/938] [D loss: 1.196711, acc: 74%] [G loss: 1.221655]\n",
      "[Epoch 0/50] [Batch 699/938] [D loss: 1.171025, acc: 82%] [G loss: 1.266842]\n",
      "[Epoch 0/50] [Batch 700/938] [D loss: 1.132109, acc: 85%] [G loss: 1.257417]\n",
      "[Epoch 0/50] [Batch 701/938] [D loss: 1.178380, acc: 78%] [G loss: 1.290919]\n",
      "[Epoch 0/50] [Batch 702/938] [D loss: 1.152141, acc: 82%] [G loss: 1.265045]\n",
      "[Epoch 0/50] [Batch 703/938] [D loss: 1.115106, acc: 87%] [G loss: 1.257062]\n",
      "[Epoch 0/50] [Batch 704/938] [D loss: 1.160014, acc: 79%] [G loss: 1.240950]\n",
      "[Epoch 0/50] [Batch 705/938] [D loss: 1.147644, acc: 78%] [G loss: 1.269624]\n",
      "[Epoch 0/50] [Batch 706/938] [D loss: 1.168968, acc: 78%] [G loss: 1.259013]\n",
      "[Epoch 0/50] [Batch 707/938] [D loss: 1.133711, acc: 86%] [G loss: 1.206252]\n",
      "[Epoch 0/50] [Batch 708/938] [D loss: 1.113138, acc: 87%] [G loss: 1.251578]\n",
      "[Epoch 0/50] [Batch 709/938] [D loss: 1.132532, acc: 82%] [G loss: 1.283602]\n",
      "[Epoch 0/50] [Batch 710/938] [D loss: 1.138441, acc: 83%] [G loss: 1.235605]\n",
      "[Epoch 0/50] [Batch 711/938] [D loss: 1.137069, acc: 83%] [G loss: 1.246826]\n",
      "[Epoch 0/50] [Batch 712/938] [D loss: 1.131624, acc: 83%] [G loss: 1.181033]\n",
      "[Epoch 0/50] [Batch 713/938] [D loss: 1.131325, acc: 86%] [G loss: 1.214839]\n",
      "[Epoch 0/50] [Batch 714/938] [D loss: 1.151062, acc: 82%] [G loss: 1.202359]\n",
      "[Epoch 0/50] [Batch 715/938] [D loss: 1.128599, acc: 85%] [G loss: 1.211213]\n",
      "[Epoch 0/50] [Batch 716/938] [D loss: 1.136169, acc: 82%] [G loss: 1.221505]\n",
      "[Epoch 0/50] [Batch 717/938] [D loss: 1.131174, acc: 89%] [G loss: 1.156875]\n",
      "[Epoch 0/50] [Batch 718/938] [D loss: 1.137287, acc: 82%] [G loss: 1.216824]\n",
      "[Epoch 0/50] [Batch 719/938] [D loss: 1.145858, acc: 80%] [G loss: 1.183260]\n",
      "[Epoch 0/50] [Batch 720/938] [D loss: 1.100864, acc: 88%] [G loss: 1.227318]\n",
      "[Epoch 0/50] [Batch 721/938] [D loss: 1.138185, acc: 84%] [G loss: 1.237361]\n",
      "[Epoch 0/50] [Batch 722/938] [D loss: 1.173825, acc: 79%] [G loss: 1.243825]\n",
      "[Epoch 0/50] [Batch 723/938] [D loss: 1.134715, acc: 81%] [G loss: 1.259331]\n",
      "[Epoch 0/50] [Batch 724/938] [D loss: 1.118217, acc: 87%] [G loss: 1.228571]\n",
      "[Epoch 0/50] [Batch 725/938] [D loss: 1.165582, acc: 78%] [G loss: 1.268817]\n",
      "[Epoch 0/50] [Batch 726/938] [D loss: 1.145020, acc: 82%] [G loss: 1.273264]\n",
      "[Epoch 0/50] [Batch 727/938] [D loss: 1.142520, acc: 87%] [G loss: 1.224980]\n",
      "[Epoch 0/50] [Batch 728/938] [D loss: 1.123382, acc: 82%] [G loss: 1.249583]\n",
      "[Epoch 0/50] [Batch 729/938] [D loss: 1.128585, acc: 83%] [G loss: 1.233044]\n",
      "[Epoch 0/50] [Batch 730/938] [D loss: 1.131476, acc: 85%] [G loss: 1.245495]\n",
      "[Epoch 0/50] [Batch 731/938] [D loss: 1.125452, acc: 89%] [G loss: 1.229293]\n",
      "[Epoch 0/50] [Batch 732/938] [D loss: 1.116052, acc: 89%] [G loss: 1.219161]\n",
      "[Epoch 0/50] [Batch 733/938] [D loss: 1.167472, acc: 79%] [G loss: 1.267657]\n",
      "[Epoch 0/50] [Batch 734/938] [D loss: 1.105943, acc: 86%] [G loss: 1.258408]\n",
      "[Epoch 0/50] [Batch 735/938] [D loss: 1.137149, acc: 84%] [G loss: 1.267460]\n",
      "[Epoch 0/50] [Batch 736/938] [D loss: 1.143431, acc: 83%] [G loss: 1.259630]\n",
      "[Epoch 0/50] [Batch 737/938] [D loss: 1.162046, acc: 78%] [G loss: 1.227542]\n",
      "[Epoch 0/50] [Batch 738/938] [D loss: 1.159617, acc: 78%] [G loss: 1.229822]\n",
      "[Epoch 0/50] [Batch 739/938] [D loss: 1.134600, acc: 84%] [G loss: 1.206954]\n",
      "[Epoch 0/50] [Batch 740/938] [D loss: 1.094073, acc: 89%] [G loss: 1.214362]\n",
      "[Epoch 0/50] [Batch 741/938] [D loss: 1.150992, acc: 82%] [G loss: 1.213290]\n",
      "[Epoch 0/50] [Batch 742/938] [D loss: 1.145424, acc: 84%] [G loss: 1.295254]\n",
      "[Epoch 0/50] [Batch 743/938] [D loss: 1.114154, acc: 89%] [G loss: 1.231924]\n",
      "[Epoch 0/50] [Batch 744/938] [D loss: 1.143351, acc: 83%] [G loss: 1.257499]\n",
      "[Epoch 0/50] [Batch 745/938] [D loss: 1.120756, acc: 83%] [G loss: 1.229147]\n",
      "[Epoch 0/50] [Batch 746/938] [D loss: 1.141841, acc: 80%] [G loss: 1.254178]\n",
      "[Epoch 0/50] [Batch 747/938] [D loss: 1.158325, acc: 79%] [G loss: 1.243149]\n",
      "[Epoch 0/50] [Batch 748/938] [D loss: 1.141348, acc: 78%] [G loss: 1.253029]\n",
      "[Epoch 0/50] [Batch 749/938] [D loss: 1.143533, acc: 82%] [G loss: 1.275229]\n",
      "[Epoch 0/50] [Batch 750/938] [D loss: 1.156251, acc: 82%] [G loss: 1.236720]\n",
      "[Epoch 0/50] [Batch 751/938] [D loss: 1.108714, acc: 86%] [G loss: 1.259855]\n",
      "[Epoch 0/50] [Batch 752/938] [D loss: 1.129471, acc: 82%] [G loss: 1.304701]\n",
      "[Epoch 0/50] [Batch 753/938] [D loss: 1.155811, acc: 82%] [G loss: 1.253370]\n",
      "[Epoch 0/50] [Batch 754/938] [D loss: 1.136655, acc: 82%] [G loss: 1.214121]\n",
      "[Epoch 0/50] [Batch 755/938] [D loss: 1.124318, acc: 88%] [G loss: 1.196777]\n",
      "[Epoch 0/50] [Batch 756/938] [D loss: 1.161444, acc: 78%] [G loss: 1.214669]\n",
      "[Epoch 0/50] [Batch 757/938] [D loss: 1.152247, acc: 83%] [G loss: 1.229710]\n",
      "[Epoch 0/50] [Batch 758/938] [D loss: 1.151006, acc: 80%] [G loss: 1.274601]\n",
      "[Epoch 0/50] [Batch 759/938] [D loss: 1.118535, acc: 82%] [G loss: 1.230568]\n",
      "[Epoch 0/50] [Batch 760/938] [D loss: 1.126647, acc: 85%] [G loss: 1.224721]\n",
      "[Epoch 0/50] [Batch 761/938] [D loss: 1.119445, acc: 86%] [G loss: 1.265324]\n",
      "[Epoch 0/50] [Batch 762/938] [D loss: 1.149888, acc: 78%] [G loss: 1.261691]\n",
      "[Epoch 0/50] [Batch 763/938] [D loss: 1.131292, acc: 83%] [G loss: 1.245438]\n",
      "[Epoch 0/50] [Batch 764/938] [D loss: 1.139495, acc: 88%] [G loss: 1.201337]\n",
      "[Epoch 0/50] [Batch 765/938] [D loss: 1.130323, acc: 85%] [G loss: 1.197241]\n",
      "[Epoch 0/50] [Batch 766/938] [D loss: 1.105313, acc: 87%] [G loss: 1.191990]\n",
      "[Epoch 0/50] [Batch 767/938] [D loss: 1.097022, acc: 90%] [G loss: 1.247215]\n",
      "[Epoch 0/50] [Batch 768/938] [D loss: 1.145743, acc: 80%] [G loss: 1.239855]\n",
      "[Epoch 0/50] [Batch 769/938] [D loss: 1.123410, acc: 85%] [G loss: 1.258385]\n",
      "[Epoch 0/50] [Batch 770/938] [D loss: 1.092562, acc: 89%] [G loss: 1.227790]\n",
      "[Epoch 0/50] [Batch 771/938] [D loss: 1.131995, acc: 82%] [G loss: 1.243460]\n",
      "[Epoch 0/50] [Batch 772/938] [D loss: 1.128765, acc: 88%] [G loss: 1.199245]\n",
      "[Epoch 0/50] [Batch 773/938] [D loss: 1.103933, acc: 89%] [G loss: 1.170684]\n",
      "[Epoch 0/50] [Batch 774/938] [D loss: 1.127945, acc: 85%] [G loss: 1.206027]\n",
      "[Epoch 0/50] [Batch 775/938] [D loss: 1.130998, acc: 78%] [G loss: 1.248681]\n",
      "[Epoch 0/50] [Batch 776/938] [D loss: 1.124240, acc: 82%] [G loss: 1.240870]\n",
      "[Epoch 0/50] [Batch 777/938] [D loss: 1.145497, acc: 79%] [G loss: 1.257519]\n",
      "[Epoch 0/50] [Batch 778/938] [D loss: 1.118769, acc: 92%] [G loss: 1.210037]\n",
      "[Epoch 0/50] [Batch 779/938] [D loss: 1.143695, acc: 83%] [G loss: 1.202089]\n",
      "[Epoch 0/50] [Batch 780/938] [D loss: 1.144301, acc: 85%] [G loss: 1.228149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/50] [Batch 781/938] [D loss: 1.139185, acc: 84%] [G loss: 1.183530]\n",
      "[Epoch 0/50] [Batch 782/938] [D loss: 1.110156, acc: 86%] [G loss: 1.238013]\n",
      "[Epoch 0/50] [Batch 783/938] [D loss: 1.124671, acc: 84%] [G loss: 1.271017]\n",
      "[Epoch 0/50] [Batch 784/938] [D loss: 1.102775, acc: 92%] [G loss: 1.245632]\n",
      "[Epoch 0/50] [Batch 785/938] [D loss: 1.097831, acc: 86%] [G loss: 1.232757]\n",
      "[Epoch 0/50] [Batch 786/938] [D loss: 1.153043, acc: 84%] [G loss: 1.204104]\n",
      "[Epoch 0/50] [Batch 787/938] [D loss: 1.145089, acc: 85%] [G loss: 1.174116]\n",
      "[Epoch 0/50] [Batch 788/938] [D loss: 1.113702, acc: 85%] [G loss: 1.227180]\n",
      "[Epoch 0/50] [Batch 789/938] [D loss: 1.121340, acc: 85%] [G loss: 1.215502]\n",
      "[Epoch 0/50] [Batch 790/938] [D loss: 1.171730, acc: 74%] [G loss: 1.263789]\n",
      "[Epoch 0/50] [Batch 791/938] [D loss: 1.117337, acc: 83%] [G loss: 1.267567]\n",
      "[Epoch 0/50] [Batch 792/938] [D loss: 1.114852, acc: 85%] [G loss: 1.208439]\n",
      "[Epoch 0/50] [Batch 793/938] [D loss: 1.098983, acc: 91%] [G loss: 1.191018]\n",
      "[Epoch 0/50] [Batch 794/938] [D loss: 1.111591, acc: 87%] [G loss: 1.148811]\n",
      "[Epoch 0/50] [Batch 795/938] [D loss: 1.145817, acc: 85%] [G loss: 1.224173]\n",
      "[Epoch 0/50] [Batch 796/938] [D loss: 1.158086, acc: 81%] [G loss: 1.204287]\n",
      "[Epoch 0/50] [Batch 797/938] [D loss: 1.086139, acc: 90%] [G loss: 1.247455]\n",
      "[Epoch 0/50] [Batch 798/938] [D loss: 1.108466, acc: 85%] [G loss: 1.261740]\n",
      "[Epoch 0/50] [Batch 799/938] [D loss: 1.141233, acc: 81%] [G loss: 1.228244]\n",
      "[Epoch 0/50] [Batch 800/938] [D loss: 1.104447, acc: 89%] [G loss: 1.174489]\n",
      "[Epoch 0/50] [Batch 801/938] [D loss: 1.126782, acc: 85%] [G loss: 1.227389]\n",
      "[Epoch 0/50] [Batch 802/938] [D loss: 1.119239, acc: 84%] [G loss: 1.237114]\n",
      "[Epoch 0/50] [Batch 803/938] [D loss: 1.109609, acc: 85%] [G loss: 1.206223]\n",
      "[Epoch 0/50] [Batch 804/938] [D loss: 1.138730, acc: 84%] [G loss: 1.244940]\n",
      "[Epoch 0/50] [Batch 805/938] [D loss: 1.119872, acc: 85%] [G loss: 1.224653]\n",
      "[Epoch 0/50] [Batch 806/938] [D loss: 1.124526, acc: 85%] [G loss: 1.238539]\n",
      "[Epoch 0/50] [Batch 807/938] [D loss: 1.157284, acc: 85%] [G loss: 1.213144]\n",
      "[Epoch 0/50] [Batch 808/938] [D loss: 1.131600, acc: 80%] [G loss: 1.216124]\n",
      "[Epoch 0/50] [Batch 809/938] [D loss: 1.138968, acc: 88%] [G loss: 1.174461]\n",
      "[Epoch 0/50] [Batch 810/938] [D loss: 1.111349, acc: 89%] [G loss: 1.200341]\n",
      "[Epoch 0/50] [Batch 811/938] [D loss: 1.127864, acc: 87%] [G loss: 1.153255]\n",
      "[Epoch 0/50] [Batch 812/938] [D loss: 1.125866, acc: 80%] [G loss: 1.217482]\n",
      "[Epoch 0/50] [Batch 813/938] [D loss: 1.100300, acc: 86%] [G loss: 1.230891]\n",
      "[Epoch 0/50] [Batch 814/938] [D loss: 1.154180, acc: 81%] [G loss: 1.228038]\n",
      "[Epoch 0/50] [Batch 815/938] [D loss: 1.130204, acc: 89%] [G loss: 1.168372]\n",
      "[Epoch 0/50] [Batch 816/938] [D loss: 1.135479, acc: 82%] [G loss: 1.210169]\n",
      "[Epoch 0/50] [Batch 817/938] [D loss: 1.187106, acc: 77%] [G loss: 1.198748]\n",
      "[Epoch 0/50] [Batch 818/938] [D loss: 1.119481, acc: 89%] [G loss: 1.207253]\n",
      "[Epoch 0/50] [Batch 819/938] [D loss: 1.131341, acc: 87%] [G loss: 1.211459]\n",
      "[Epoch 0/50] [Batch 820/938] [D loss: 1.124143, acc: 88%] [G loss: 1.207095]\n",
      "[Epoch 0/50] [Batch 821/938] [D loss: 1.121469, acc: 88%] [G loss: 1.192134]\n",
      "[Epoch 0/50] [Batch 822/938] [D loss: 1.128383, acc: 84%] [G loss: 1.230691]\n",
      "[Epoch 0/50] [Batch 823/938] [D loss: 1.114800, acc: 84%] [G loss: 1.243762]\n",
      "[Epoch 0/50] [Batch 824/938] [D loss: 1.139333, acc: 86%] [G loss: 1.193049]\n",
      "[Epoch 0/50] [Batch 825/938] [D loss: 1.120708, acc: 85%] [G loss: 1.215969]\n",
      "[Epoch 0/50] [Batch 826/938] [D loss: 1.132320, acc: 87%] [G loss: 1.218106]\n",
      "[Epoch 0/50] [Batch 827/938] [D loss: 1.098836, acc: 87%] [G loss: 1.245500]\n",
      "[Epoch 0/50] [Batch 828/938] [D loss: 1.087673, acc: 89%] [G loss: 1.208307]\n",
      "[Epoch 0/50] [Batch 829/938] [D loss: 1.124102, acc: 84%] [G loss: 1.208626]\n",
      "[Epoch 0/50] [Batch 830/938] [D loss: 1.131743, acc: 85%] [G loss: 1.220939]\n",
      "[Epoch 0/50] [Batch 831/938] [D loss: 1.139595, acc: 81%] [G loss: 1.266851]\n",
      "[Epoch 0/50] [Batch 832/938] [D loss: 1.104571, acc: 86%] [G loss: 1.179461]\n",
      "[Epoch 0/50] [Batch 833/938] [D loss: 1.129798, acc: 85%] [G loss: 1.179674]\n",
      "[Epoch 0/50] [Batch 834/938] [D loss: 1.142102, acc: 85%] [G loss: 1.191510]\n",
      "[Epoch 0/50] [Batch 835/938] [D loss: 1.143422, acc: 82%] [G loss: 1.161739]\n",
      "[Epoch 0/50] [Batch 836/938] [D loss: 1.116286, acc: 87%] [G loss: 1.214492]\n",
      "[Epoch 0/50] [Batch 837/938] [D loss: 1.129338, acc: 86%] [G loss: 1.203372]\n",
      "[Epoch 0/50] [Batch 838/938] [D loss: 1.133643, acc: 85%] [G loss: 1.242226]\n",
      "[Epoch 0/50] [Batch 839/938] [D loss: 1.153666, acc: 79%] [G loss: 1.235946]\n",
      "[Epoch 0/50] [Batch 840/938] [D loss: 1.102531, acc: 88%] [G loss: 1.194731]\n",
      "[Epoch 0/50] [Batch 841/938] [D loss: 1.121873, acc: 85%] [G loss: 1.227484]\n",
      "[Epoch 0/50] [Batch 842/938] [D loss: 1.115710, acc: 87%] [G loss: 1.218729]\n",
      "[Epoch 0/50] [Batch 843/938] [D loss: 1.104419, acc: 87%] [G loss: 1.209577]\n",
      "[Epoch 0/50] [Batch 844/938] [D loss: 1.150203, acc: 85%] [G loss: 1.191649]\n",
      "[Epoch 0/50] [Batch 845/938] [D loss: 1.111443, acc: 91%] [G loss: 1.195642]\n",
      "[Epoch 0/50] [Batch 846/938] [D loss: 1.112324, acc: 84%] [G loss: 1.228347]\n",
      "[Epoch 0/50] [Batch 847/938] [D loss: 1.105003, acc: 93%] [G loss: 1.131672]\n",
      "[Epoch 0/50] [Batch 848/938] [D loss: 1.123363, acc: 87%] [G loss: 1.191097]\n",
      "[Epoch 0/50] [Batch 849/938] [D loss: 1.103890, acc: 85%] [G loss: 1.258480]\n",
      "[Epoch 0/50] [Batch 850/938] [D loss: 1.141340, acc: 84%] [G loss: 1.232960]\n",
      "[Epoch 0/50] [Batch 851/938] [D loss: 1.123254, acc: 87%] [G loss: 1.217001]\n",
      "[Epoch 0/50] [Batch 852/938] [D loss: 1.097452, acc: 88%] [G loss: 1.201790]\n",
      "[Epoch 0/50] [Batch 853/938] [D loss: 1.098899, acc: 92%] [G loss: 1.155527]\n",
      "[Epoch 0/50] [Batch 854/938] [D loss: 1.145103, acc: 85%] [G loss: 1.177792]\n",
      "[Epoch 0/50] [Batch 855/938] [D loss: 1.117566, acc: 87%] [G loss: 1.202269]\n",
      "[Epoch 0/50] [Batch 856/938] [D loss: 1.127508, acc: 85%] [G loss: 1.153864]\n",
      "[Epoch 0/50] [Batch 857/938] [D loss: 1.132628, acc: 82%] [G loss: 1.258267]\n",
      "[Epoch 0/50] [Batch 858/938] [D loss: 1.119054, acc: 85%] [G loss: 1.250615]\n",
      "[Epoch 0/50] [Batch 859/938] [D loss: 1.149346, acc: 83%] [G loss: 1.196291]\n",
      "[Epoch 0/50] [Batch 860/938] [D loss: 1.090444, acc: 89%] [G loss: 1.218519]\n",
      "[Epoch 0/50] [Batch 861/938] [D loss: 1.134340, acc: 82%] [G loss: 1.260023]\n",
      "[Epoch 0/50] [Batch 862/938] [D loss: 1.134185, acc: 84%] [G loss: 1.204102]\n",
      "[Epoch 0/50] [Batch 863/938] [D loss: 1.108569, acc: 89%] [G loss: 1.244973]\n",
      "[Epoch 0/50] [Batch 864/938] [D loss: 1.125396, acc: 89%] [G loss: 1.222134]\n",
      "[Epoch 0/50] [Batch 865/938] [D loss: 1.122984, acc: 85%] [G loss: 1.227107]\n",
      "[Epoch 0/50] [Batch 866/938] [D loss: 1.113301, acc: 85%] [G loss: 1.252723]\n",
      "[Epoch 0/50] [Batch 867/938] [D loss: 1.124301, acc: 84%] [G loss: 1.187580]\n",
      "[Epoch 0/50] [Batch 868/938] [D loss: 1.147990, acc: 85%] [G loss: 1.221114]\n",
      "[Epoch 0/50] [Batch 869/938] [D loss: 1.117600, acc: 88%] [G loss: 1.180074]\n",
      "[Epoch 0/50] [Batch 870/938] [D loss: 1.110185, acc: 86%] [G loss: 1.137300]\n",
      "[Epoch 0/50] [Batch 871/938] [D loss: 1.097603, acc: 89%] [G loss: 1.171532]\n",
      "[Epoch 0/50] [Batch 872/938] [D loss: 1.137901, acc: 85%] [G loss: 1.236420]\n",
      "[Epoch 0/50] [Batch 873/938] [D loss: 1.117326, acc: 88%] [G loss: 1.297047]\n",
      "[Epoch 0/50] [Batch 874/938] [D loss: 1.096820, acc: 84%] [G loss: 1.187457]\n",
      "[Epoch 0/50] [Batch 875/938] [D loss: 1.101367, acc: 91%] [G loss: 1.189994]\n",
      "[Epoch 0/50] [Batch 876/938] [D loss: 1.102485, acc: 90%] [G loss: 1.213514]\n",
      "[Epoch 0/50] [Batch 877/938] [D loss: 1.146285, acc: 84%] [G loss: 1.171641]\n",
      "[Epoch 0/50] [Batch 878/938] [D loss: 1.128635, acc: 80%] [G loss: 1.209442]\n",
      "[Epoch 0/50] [Batch 879/938] [D loss: 1.128460, acc: 85%] [G loss: 1.168324]\n",
      "[Epoch 0/50] [Batch 880/938] [D loss: 1.114618, acc: 89%] [G loss: 1.235306]\n",
      "[Epoch 0/50] [Batch 881/938] [D loss: 1.129463, acc: 83%] [G loss: 1.253223]\n",
      "[Epoch 0/50] [Batch 882/938] [D loss: 1.107594, acc: 90%] [G loss: 1.197355]\n",
      "[Epoch 0/50] [Batch 883/938] [D loss: 1.140527, acc: 84%] [G loss: 1.163004]\n",
      "[Epoch 0/50] [Batch 884/938] [D loss: 1.111898, acc: 87%] [G loss: 1.171503]\n",
      "[Epoch 0/50] [Batch 885/938] [D loss: 1.086236, acc: 87%] [G loss: 1.216362]\n",
      "[Epoch 0/50] [Batch 886/938] [D loss: 1.099895, acc: 88%] [G loss: 1.167665]\n",
      "[Epoch 0/50] [Batch 887/938] [D loss: 1.098973, acc: 92%] [G loss: 1.210631]\n",
      "[Epoch 0/50] [Batch 888/938] [D loss: 1.109487, acc: 85%] [G loss: 1.225430]\n",
      "[Epoch 0/50] [Batch 889/938] [D loss: 1.104169, acc: 89%] [G loss: 1.213390]\n",
      "[Epoch 0/50] [Batch 890/938] [D loss: 1.111927, acc: 92%] [G loss: 1.198531]\n",
      "[Epoch 0/50] [Batch 891/938] [D loss: 1.105504, acc: 88%] [G loss: 1.226574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/50] [Batch 892/938] [D loss: 1.114382, acc: 90%] [G loss: 1.210890]\n",
      "[Epoch 0/50] [Batch 893/938] [D loss: 1.093527, acc: 90%] [G loss: 1.239601]\n",
      "[Epoch 0/50] [Batch 894/938] [D loss: 1.111109, acc: 90%] [G loss: 1.184512]\n",
      "[Epoch 0/50] [Batch 895/938] [D loss: 1.104240, acc: 90%] [G loss: 1.169980]\n",
      "[Epoch 0/50] [Batch 896/938] [D loss: 1.083358, acc: 88%] [G loss: 1.211347]\n",
      "[Epoch 0/50] [Batch 897/938] [D loss: 1.119747, acc: 87%] [G loss: 1.196724]\n",
      "[Epoch 0/50] [Batch 898/938] [D loss: 1.112629, acc: 89%] [G loss: 1.215811]\n",
      "[Epoch 0/50] [Batch 899/938] [D loss: 1.121751, acc: 84%] [G loss: 1.246258]\n",
      "[Epoch 0/50] [Batch 900/938] [D loss: 1.119949, acc: 88%] [G loss: 1.173769]\n",
      "[Epoch 0/50] [Batch 901/938] [D loss: 1.125484, acc: 85%] [G loss: 1.259220]\n",
      "[Epoch 0/50] [Batch 902/938] [D loss: 1.106081, acc: 87%] [G loss: 1.223268]\n",
      "[Epoch 0/50] [Batch 903/938] [D loss: 1.111878, acc: 90%] [G loss: 1.251742]\n",
      "[Epoch 0/50] [Batch 904/938] [D loss: 1.124807, acc: 87%] [G loss: 1.242568]\n",
      "[Epoch 0/50] [Batch 905/938] [D loss: 1.114698, acc: 90%] [G loss: 1.166083]\n",
      "[Epoch 0/50] [Batch 906/938] [D loss: 1.102808, acc: 94%] [G loss: 1.203381]\n",
      "[Epoch 0/50] [Batch 907/938] [D loss: 1.095151, acc: 90%] [G loss: 1.178026]\n",
      "[Epoch 0/50] [Batch 908/938] [D loss: 1.124975, acc: 84%] [G loss: 1.238588]\n",
      "[Epoch 0/50] [Batch 909/938] [D loss: 1.147925, acc: 82%] [G loss: 1.206708]\n",
      "[Epoch 0/50] [Batch 910/938] [D loss: 1.101887, acc: 90%] [G loss: 1.275479]\n",
      "[Epoch 0/50] [Batch 911/938] [D loss: 1.130549, acc: 89%] [G loss: 1.237757]\n",
      "[Epoch 0/50] [Batch 912/938] [D loss: 1.123387, acc: 87%] [G loss: 1.244156]\n",
      "[Epoch 0/50] [Batch 913/938] [D loss: 1.133445, acc: 82%] [G loss: 1.213408]\n",
      "[Epoch 0/50] [Batch 914/938] [D loss: 1.132694, acc: 83%] [G loss: 1.184435]\n",
      "[Epoch 0/50] [Batch 915/938] [D loss: 1.109951, acc: 92%] [G loss: 1.185797]\n",
      "[Epoch 0/50] [Batch 916/938] [D loss: 1.090903, acc: 87%] [G loss: 1.239259]\n",
      "[Epoch 0/50] [Batch 917/938] [D loss: 1.128584, acc: 85%] [G loss: 1.224133]\n",
      "[Epoch 0/50] [Batch 918/938] [D loss: 1.135101, acc: 83%] [G loss: 1.166419]\n",
      "[Epoch 0/50] [Batch 919/938] [D loss: 1.107348, acc: 89%] [G loss: 1.214519]\n",
      "[Epoch 0/50] [Batch 920/938] [D loss: 1.097921, acc: 89%] [G loss: 1.157446]\n",
      "[Epoch 0/50] [Batch 921/938] [D loss: 1.129806, acc: 86%] [G loss: 1.182906]\n",
      "[Epoch 0/50] [Batch 922/938] [D loss: 1.116739, acc: 88%] [G loss: 1.227257]\n",
      "[Epoch 0/50] [Batch 923/938] [D loss: 1.063348, acc: 90%] [G loss: 1.253066]\n",
      "[Epoch 0/50] [Batch 924/938] [D loss: 1.051709, acc: 95%] [G loss: 1.231504]\n",
      "[Epoch 0/50] [Batch 925/938] [D loss: 1.152063, acc: 81%] [G loss: 1.233428]\n",
      "[Epoch 0/50] [Batch 926/938] [D loss: 1.126383, acc: 88%] [G loss: 1.193423]\n",
      "[Epoch 0/50] [Batch 927/938] [D loss: 1.108342, acc: 85%] [G loss: 1.225075]\n",
      "[Epoch 0/50] [Batch 928/938] [D loss: 1.122740, acc: 84%] [G loss: 1.230386]\n",
      "[Epoch 0/50] [Batch 929/938] [D loss: 1.120691, acc: 87%] [G loss: 1.151320]\n",
      "[Epoch 0/50] [Batch 930/938] [D loss: 1.093876, acc: 88%] [G loss: 1.190572]\n",
      "[Epoch 0/50] [Batch 931/938] [D loss: 1.084348, acc: 92%] [G loss: 1.226276]\n",
      "[Epoch 0/50] [Batch 932/938] [D loss: 1.119507, acc: 89%] [G loss: 1.191186]\n",
      "[Epoch 0/50] [Batch 933/938] [D loss: 1.103966, acc: 90%] [G loss: 1.224038]\n",
      "[Epoch 0/50] [Batch 934/938] [D loss: 1.074581, acc: 92%] [G loss: 1.151173]\n",
      "[Epoch 0/50] [Batch 935/938] [D loss: 1.152832, acc: 82%] [G loss: 1.206541]\n",
      "[Epoch 0/50] [Batch 936/938] [D loss: 1.134235, acc: 85%] [G loss: 1.171528]\n",
      "[Epoch 0/50] [Batch 937/938] [D loss: 1.111711, acc: 85%] [G loss: 1.194127]\n",
      "[Epoch 1/50] [Batch 0/938] [D loss: 1.125767, acc: 85%] [G loss: 1.226616]\n",
      "[Epoch 1/50] [Batch 1/938] [D loss: 1.093888, acc: 91%] [G loss: 1.178445]\n",
      "[Epoch 1/50] [Batch 2/938] [D loss: 1.114792, acc: 87%] [G loss: 1.163305]\n",
      "[Epoch 1/50] [Batch 3/938] [D loss: 1.087759, acc: 93%] [G loss: 1.152630]\n",
      "[Epoch 1/50] [Batch 4/938] [D loss: 1.099007, acc: 91%] [G loss: 1.204630]\n",
      "[Epoch 1/50] [Batch 5/938] [D loss: 1.094722, acc: 94%] [G loss: 1.206623]\n",
      "[Epoch 1/50] [Batch 6/938] [D loss: 1.114000, acc: 89%] [G loss: 1.207964]\n",
      "[Epoch 1/50] [Batch 7/938] [D loss: 1.083368, acc: 89%] [G loss: 1.185424]\n",
      "[Epoch 1/50] [Batch 8/938] [D loss: 1.134596, acc: 88%] [G loss: 1.207835]\n",
      "[Epoch 1/50] [Batch 9/938] [D loss: 1.108454, acc: 87%] [G loss: 1.197719]\n",
      "[Epoch 1/50] [Batch 10/938] [D loss: 1.125635, acc: 84%] [G loss: 1.257084]\n",
      "[Epoch 1/50] [Batch 11/938] [D loss: 1.108820, acc: 90%] [G loss: 1.147635]\n",
      "[Epoch 1/50] [Batch 12/938] [D loss: 1.097258, acc: 92%] [G loss: 1.214521]\n",
      "[Epoch 1/50] [Batch 13/938] [D loss: 1.105799, acc: 88%] [G loss: 1.179984]\n",
      "[Epoch 1/50] [Batch 14/938] [D loss: 1.093219, acc: 92%] [G loss: 1.187236]\n",
      "[Epoch 1/50] [Batch 15/938] [D loss: 1.108562, acc: 89%] [G loss: 1.198043]\n",
      "[Epoch 1/50] [Batch 16/938] [D loss: 1.107851, acc: 89%] [G loss: 1.219444]\n",
      "[Epoch 1/50] [Batch 17/938] [D loss: 1.081639, acc: 89%] [G loss: 1.231167]\n",
      "[Epoch 1/50] [Batch 18/938] [D loss: 1.126519, acc: 85%] [G loss: 1.221526]\n",
      "[Epoch 1/50] [Batch 19/938] [D loss: 1.119939, acc: 90%] [G loss: 1.184240]\n",
      "[Epoch 1/50] [Batch 20/938] [D loss: 1.102220, acc: 90%] [G loss: 1.199088]\n",
      "[Epoch 1/50] [Batch 21/938] [D loss: 1.151505, acc: 85%] [G loss: 1.186851]\n",
      "[Epoch 1/50] [Batch 22/938] [D loss: 1.092612, acc: 91%] [G loss: 1.147234]\n",
      "[Epoch 1/50] [Batch 23/938] [D loss: 1.128884, acc: 88%] [G loss: 1.196289]\n",
      "[Epoch 1/50] [Batch 24/938] [D loss: 1.136139, acc: 86%] [G loss: 1.199504]\n",
      "[Epoch 1/50] [Batch 25/938] [D loss: 1.118095, acc: 88%] [G loss: 1.186329]\n",
      "[Epoch 1/50] [Batch 26/938] [D loss: 1.128017, acc: 85%] [G loss: 1.259607]\n",
      "[Epoch 1/50] [Batch 27/938] [D loss: 1.090556, acc: 91%] [G loss: 1.197312]\n",
      "[Epoch 1/50] [Batch 28/938] [D loss: 1.095960, acc: 88%] [G loss: 1.198561]\n",
      "[Epoch 1/50] [Batch 29/938] [D loss: 1.133962, acc: 89%] [G loss: 1.129614]\n",
      "[Epoch 1/50] [Batch 30/938] [D loss: 1.090124, acc: 92%] [G loss: 1.157776]\n",
      "[Epoch 1/50] [Batch 31/938] [D loss: 1.109799, acc: 89%] [G loss: 1.202197]\n",
      "[Epoch 1/50] [Batch 32/938] [D loss: 1.089617, acc: 93%] [G loss: 1.180463]\n",
      "[Epoch 1/50] [Batch 33/938] [D loss: 1.115419, acc: 86%] [G loss: 1.178127]\n",
      "[Epoch 1/50] [Batch 34/938] [D loss: 1.108029, acc: 91%] [G loss: 1.175102]\n",
      "[Epoch 1/50] [Batch 35/938] [D loss: 1.118306, acc: 87%] [G loss: 1.222721]\n",
      "[Epoch 1/50] [Batch 36/938] [D loss: 1.126530, acc: 86%] [G loss: 1.185088]\n",
      "[Epoch 1/50] [Batch 37/938] [D loss: 1.097848, acc: 92%] [G loss: 1.174500]\n",
      "[Epoch 1/50] [Batch 38/938] [D loss: 1.128407, acc: 88%] [G loss: 1.163408]\n",
      "[Epoch 1/50] [Batch 39/938] [D loss: 1.100706, acc: 90%] [G loss: 1.218443]\n",
      "[Epoch 1/50] [Batch 40/938] [D loss: 1.120883, acc: 89%] [G loss: 1.179386]\n",
      "[Epoch 1/50] [Batch 41/938] [D loss: 1.081917, acc: 89%] [G loss: 1.169383]\n",
      "[Epoch 1/50] [Batch 42/938] [D loss: 1.110815, acc: 89%] [G loss: 1.227360]\n",
      "[Epoch 1/50] [Batch 43/938] [D loss: 1.108630, acc: 88%] [G loss: 1.190288]\n",
      "[Epoch 1/50] [Batch 44/938] [D loss: 1.120758, acc: 87%] [G loss: 1.215286]\n",
      "[Epoch 1/50] [Batch 45/938] [D loss: 1.151086, acc: 85%] [G loss: 1.218188]\n",
      "[Epoch 1/50] [Batch 46/938] [D loss: 1.112982, acc: 88%] [G loss: 1.221809]\n",
      "[Epoch 1/50] [Batch 47/938] [D loss: 1.134718, acc: 86%] [G loss: 1.162203]\n",
      "[Epoch 1/50] [Batch 48/938] [D loss: 1.101216, acc: 88%] [G loss: 1.213056]\n",
      "[Epoch 1/50] [Batch 49/938] [D loss: 1.106005, acc: 91%] [G loss: 1.186402]\n",
      "[Epoch 1/50] [Batch 50/938] [D loss: 1.142382, acc: 86%] [G loss: 1.162122]\n",
      "[Epoch 1/50] [Batch 51/938] [D loss: 1.112740, acc: 87%] [G loss: 1.208471]\n",
      "[Epoch 1/50] [Batch 52/938] [D loss: 1.141710, acc: 86%] [G loss: 1.201216]\n",
      "[Epoch 1/50] [Batch 53/938] [D loss: 1.105248, acc: 91%] [G loss: 1.119893]\n",
      "[Epoch 1/50] [Batch 54/938] [D loss: 1.111225, acc: 88%] [G loss: 1.215067]\n",
      "[Epoch 1/50] [Batch 55/938] [D loss: 1.119425, acc: 85%] [G loss: 1.243160]\n",
      "[Epoch 1/50] [Batch 56/938] [D loss: 1.099227, acc: 90%] [G loss: 1.217032]\n",
      "[Epoch 1/50] [Batch 57/938] [D loss: 1.081242, acc: 89%] [G loss: 1.269343]\n",
      "[Epoch 1/50] [Batch 58/938] [D loss: 1.081903, acc: 92%] [G loss: 1.202026]\n",
      "[Epoch 1/50] [Batch 59/938] [D loss: 1.092876, acc: 92%] [G loss: 1.191025]\n",
      "[Epoch 1/50] [Batch 60/938] [D loss: 1.141203, acc: 86%] [G loss: 1.237402]\n",
      "[Epoch 1/50] [Batch 61/938] [D loss: 1.125154, acc: 87%] [G loss: 1.257083]\n",
      "[Epoch 1/50] [Batch 62/938] [D loss: 1.106361, acc: 89%] [G loss: 1.122265]\n",
      "[Epoch 1/50] [Batch 63/938] [D loss: 1.109906, acc: 88%] [G loss: 1.223970]\n",
      "[Epoch 1/50] [Batch 64/938] [D loss: 1.107406, acc: 88%] [G loss: 1.190022]\n",
      "[Epoch 1/50] [Batch 65/938] [D loss: 1.133216, acc: 83%] [G loss: 1.228977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/50] [Batch 66/938] [D loss: 1.114855, acc: 88%] [G loss: 1.194634]\n",
      "[Epoch 1/50] [Batch 67/938] [D loss: 1.103023, acc: 91%] [G loss: 1.149803]\n",
      "[Epoch 1/50] [Batch 68/938] [D loss: 1.122215, acc: 88%] [G loss: 1.186125]\n",
      "[Epoch 1/50] [Batch 69/938] [D loss: 1.103862, acc: 85%] [G loss: 1.205298]\n",
      "[Epoch 1/50] [Batch 70/938] [D loss: 1.122338, acc: 93%] [G loss: 1.190117]\n",
      "[Epoch 1/50] [Batch 71/938] [D loss: 1.131681, acc: 82%] [G loss: 1.302590]\n",
      "[Epoch 1/50] [Batch 72/938] [D loss: 1.094315, acc: 87%] [G loss: 1.179724]\n",
      "[Epoch 1/50] [Batch 73/938] [D loss: 1.103163, acc: 89%] [G loss: 1.216027]\n",
      "[Epoch 1/50] [Batch 74/938] [D loss: 1.109790, acc: 91%] [G loss: 1.169244]\n",
      "[Epoch 1/50] [Batch 75/938] [D loss: 1.101091, acc: 88%] [G loss: 1.221270]\n",
      "[Epoch 1/50] [Batch 76/938] [D loss: 1.100112, acc: 90%] [G loss: 1.223912]\n",
      "[Epoch 1/50] [Batch 77/938] [D loss: 1.099638, acc: 90%] [G loss: 1.207265]\n",
      "[Epoch 1/50] [Batch 78/938] [D loss: 1.117592, acc: 86%] [G loss: 1.171158]\n",
      "[Epoch 1/50] [Batch 79/938] [D loss: 1.089686, acc: 91%] [G loss: 1.178967]\n",
      "[Epoch 1/50] [Batch 80/938] [D loss: 1.104483, acc: 87%] [G loss: 1.180593]\n",
      "[Epoch 1/50] [Batch 81/938] [D loss: 1.123232, acc: 87%] [G loss: 1.194999]\n",
      "[Epoch 1/50] [Batch 82/938] [D loss: 1.083354, acc: 92%] [G loss: 1.182406]\n",
      "[Epoch 1/50] [Batch 83/938] [D loss: 1.110966, acc: 88%] [G loss: 1.185595]\n",
      "[Epoch 1/50] [Batch 84/938] [D loss: 1.120279, acc: 85%] [G loss: 1.169132]\n",
      "[Epoch 1/50] [Batch 85/938] [D loss: 1.104129, acc: 88%] [G loss: 1.170323]\n",
      "[Epoch 1/50] [Batch 86/938] [D loss: 1.115110, acc: 85%] [G loss: 1.181042]\n",
      "[Epoch 1/50] [Batch 87/938] [D loss: 1.103376, acc: 89%] [G loss: 1.175156]\n",
      "[Epoch 1/50] [Batch 88/938] [D loss: 1.109392, acc: 91%] [G loss: 1.108523]\n",
      "[Epoch 1/50] [Batch 89/938] [D loss: 1.102109, acc: 85%] [G loss: 1.200517]\n",
      "[Epoch 1/50] [Batch 90/938] [D loss: 1.091716, acc: 91%] [G loss: 1.227925]\n",
      "[Epoch 1/50] [Batch 91/938] [D loss: 1.092749, acc: 90%] [G loss: 1.155866]\n",
      "[Epoch 1/50] [Batch 92/938] [D loss: 1.124779, acc: 86%] [G loss: 1.221381]\n",
      "[Epoch 1/50] [Batch 93/938] [D loss: 1.084968, acc: 94%] [G loss: 1.171084]\n",
      "[Epoch 1/50] [Batch 94/938] [D loss: 1.133707, acc: 89%] [G loss: 1.183758]\n",
      "[Epoch 1/50] [Batch 95/938] [D loss: 1.132178, acc: 89%] [G loss: 1.174197]\n",
      "[Epoch 1/50] [Batch 96/938] [D loss: 1.097167, acc: 89%] [G loss: 1.151284]\n",
      "[Epoch 1/50] [Batch 97/938] [D loss: 1.114725, acc: 89%] [G loss: 1.167406]\n",
      "[Epoch 1/50] [Batch 98/938] [D loss: 1.110594, acc: 87%] [G loss: 1.169728]\n",
      "[Epoch 1/50] [Batch 99/938] [D loss: 1.095587, acc: 88%] [G loss: 1.190485]\n",
      "[Epoch 1/50] [Batch 100/938] [D loss: 1.101990, acc: 89%] [G loss: 1.184857]\n",
      "[Epoch 1/50] [Batch 101/938] [D loss: 1.137531, acc: 82%] [G loss: 1.222936]\n",
      "[Epoch 1/50] [Batch 102/938] [D loss: 1.083032, acc: 92%] [G loss: 1.158976]\n",
      "[Epoch 1/50] [Batch 103/938] [D loss: 1.108202, acc: 86%] [G loss: 1.231826]\n",
      "[Epoch 1/50] [Batch 104/938] [D loss: 1.104634, acc: 91%] [G loss: 1.224536]\n",
      "[Epoch 1/50] [Batch 105/938] [D loss: 1.096439, acc: 91%] [G loss: 1.220144]\n",
      "[Epoch 1/50] [Batch 106/938] [D loss: 1.100043, acc: 95%] [G loss: 1.211242]\n",
      "[Epoch 1/50] [Batch 107/938] [D loss: 1.107082, acc: 89%] [G loss: 1.206921]\n",
      "[Epoch 1/50] [Batch 108/938] [D loss: 1.105348, acc: 85%] [G loss: 1.216572]\n",
      "[Epoch 1/50] [Batch 109/938] [D loss: 1.122520, acc: 85%] [G loss: 1.158276]\n",
      "[Epoch 1/50] [Batch 110/938] [D loss: 1.120690, acc: 90%] [G loss: 1.172830]\n",
      "[Epoch 1/50] [Batch 111/938] [D loss: 1.089796, acc: 87%] [G loss: 1.164119]\n",
      "[Epoch 1/50] [Batch 112/938] [D loss: 1.124026, acc: 90%] [G loss: 1.213252]\n",
      "[Epoch 1/50] [Batch 113/938] [D loss: 1.120702, acc: 92%] [G loss: 1.204292]\n",
      "[Epoch 1/50] [Batch 114/938] [D loss: 1.099431, acc: 89%] [G loss: 1.187645]\n",
      "[Epoch 1/50] [Batch 115/938] [D loss: 1.117124, acc: 90%] [G loss: 1.230232]\n",
      "[Epoch 1/50] [Batch 116/938] [D loss: 1.116299, acc: 87%] [G loss: 1.167116]\n",
      "[Epoch 1/50] [Batch 117/938] [D loss: 1.095062, acc: 85%] [G loss: 1.201177]\n",
      "[Epoch 1/50] [Batch 118/938] [D loss: 1.103911, acc: 89%] [G loss: 1.181403]\n",
      "[Epoch 1/50] [Batch 119/938] [D loss: 1.113922, acc: 88%] [G loss: 1.184060]\n",
      "[Epoch 1/50] [Batch 120/938] [D loss: 1.096800, acc: 91%] [G loss: 1.181999]\n",
      "[Epoch 1/50] [Batch 121/938] [D loss: 1.116138, acc: 89%] [G loss: 1.186083]\n",
      "[Epoch 1/50] [Batch 122/938] [D loss: 1.102466, acc: 89%] [G loss: 1.191080]\n",
      "[Epoch 1/50] [Batch 123/938] [D loss: 1.078687, acc: 93%] [G loss: 1.178111]\n",
      "[Epoch 1/50] [Batch 124/938] [D loss: 1.138023, acc: 85%] [G loss: 1.166801]\n",
      "[Epoch 1/50] [Batch 125/938] [D loss: 1.123917, acc: 87%] [G loss: 1.164779]\n",
      "[Epoch 1/50] [Batch 126/938] [D loss: 1.104707, acc: 82%] [G loss: 1.229471]\n",
      "[Epoch 1/50] [Batch 127/938] [D loss: 1.108155, acc: 90%] [G loss: 1.176000]\n",
      "[Epoch 1/50] [Batch 128/938] [D loss: 1.075617, acc: 93%] [G loss: 1.189129]\n",
      "[Epoch 1/50] [Batch 129/938] [D loss: 1.097366, acc: 86%] [G loss: 1.195573]\n",
      "[Epoch 1/50] [Batch 130/938] [D loss: 1.086042, acc: 90%] [G loss: 1.217164]\n",
      "[Epoch 1/50] [Batch 131/938] [D loss: 1.163993, acc: 86%] [G loss: 1.134305]\n",
      "[Epoch 1/50] [Batch 132/938] [D loss: 1.133415, acc: 92%] [G loss: 1.170702]\n",
      "[Epoch 1/50] [Batch 133/938] [D loss: 1.097735, acc: 89%] [G loss: 1.179279]\n",
      "[Epoch 1/50] [Batch 134/938] [D loss: 1.105347, acc: 91%] [G loss: 1.175644]\n",
      "[Epoch 1/50] [Batch 135/938] [D loss: 1.094467, acc: 88%] [G loss: 1.203192]\n",
      "[Epoch 1/50] [Batch 136/938] [D loss: 1.113846, acc: 90%] [G loss: 1.227856]\n",
      "[Epoch 1/50] [Batch 137/938] [D loss: 1.116227, acc: 88%] [G loss: 1.245879]\n",
      "[Epoch 1/50] [Batch 138/938] [D loss: 1.100860, acc: 90%] [G loss: 1.176192]\n",
      "[Epoch 1/50] [Batch 139/938] [D loss: 1.125135, acc: 88%] [G loss: 1.190225]\n",
      "[Epoch 1/50] [Batch 140/938] [D loss: 1.110351, acc: 89%] [G loss: 1.221757]\n",
      "[Epoch 1/50] [Batch 141/938] [D loss: 1.089688, acc: 92%] [G loss: 1.158652]\n",
      "[Epoch 1/50] [Batch 142/938] [D loss: 1.080808, acc: 93%] [G loss: 1.198382]\n",
      "[Epoch 1/50] [Batch 143/938] [D loss: 1.101369, acc: 87%] [G loss: 1.186099]\n",
      "[Epoch 1/50] [Batch 144/938] [D loss: 1.114853, acc: 91%] [G loss: 1.194556]\n",
      "[Epoch 1/50] [Batch 145/938] [D loss: 1.104440, acc: 90%] [G loss: 1.193271]\n",
      "[Epoch 1/50] [Batch 146/938] [D loss: 1.108305, acc: 88%] [G loss: 1.137477]\n",
      "[Epoch 1/50] [Batch 147/938] [D loss: 1.100313, acc: 89%] [G loss: 1.217937]\n",
      "[Epoch 1/50] [Batch 148/938] [D loss: 1.095484, acc: 91%] [G loss: 1.174714]\n",
      "[Epoch 1/50] [Batch 149/938] [D loss: 1.092787, acc: 92%] [G loss: 1.192066]\n",
      "[Epoch 1/50] [Batch 150/938] [D loss: 1.113273, acc: 93%] [G loss: 1.108324]\n",
      "[Epoch 1/50] [Batch 151/938] [D loss: 1.094172, acc: 90%] [G loss: 1.178846]\n",
      "[Epoch 1/50] [Batch 152/938] [D loss: 1.121989, acc: 90%] [G loss: 1.178207]\n",
      "[Epoch 1/50] [Batch 153/938] [D loss: 1.091686, acc: 86%] [G loss: 1.212193]\n",
      "[Epoch 1/50] [Batch 154/938] [D loss: 1.113332, acc: 89%] [G loss: 1.224859]\n",
      "[Epoch 1/50] [Batch 155/938] [D loss: 1.085781, acc: 90%] [G loss: 1.178522]\n",
      "[Epoch 1/50] [Batch 156/938] [D loss: 1.093857, acc: 92%] [G loss: 1.193144]\n",
      "[Epoch 1/50] [Batch 157/938] [D loss: 1.121380, acc: 85%] [G loss: 1.175395]\n",
      "[Epoch 1/50] [Batch 158/938] [D loss: 1.122249, acc: 86%] [G loss: 1.143625]\n",
      "[Epoch 1/50] [Batch 159/938] [D loss: 1.103835, acc: 93%] [G loss: 1.148413]\n",
      "[Epoch 1/50] [Batch 160/938] [D loss: 1.105779, acc: 87%] [G loss: 1.215211]\n",
      "[Epoch 1/50] [Batch 161/938] [D loss: 1.068831, acc: 95%] [G loss: 1.164585]\n",
      "[Epoch 1/50] [Batch 162/938] [D loss: 1.110633, acc: 89%] [G loss: 1.250257]\n",
      "[Epoch 1/50] [Batch 163/938] [D loss: 1.117598, acc: 92%] [G loss: 1.158096]\n",
      "[Epoch 1/50] [Batch 164/938] [D loss: 1.128093, acc: 88%] [G loss: 1.186273]\n",
      "[Epoch 1/50] [Batch 165/938] [D loss: 1.107333, acc: 89%] [G loss: 1.213496]\n",
      "[Epoch 1/50] [Batch 166/938] [D loss: 1.101336, acc: 90%] [G loss: 1.165064]\n",
      "[Epoch 1/50] [Batch 167/938] [D loss: 1.108580, acc: 87%] [G loss: 1.189995]\n",
      "[Epoch 1/50] [Batch 168/938] [D loss: 1.118911, acc: 92%] [G loss: 1.149073]\n",
      "[Epoch 1/50] [Batch 169/938] [D loss: 1.086814, acc: 93%] [G loss: 1.210180]\n",
      "[Epoch 1/50] [Batch 170/938] [D loss: 1.127540, acc: 89%] [G loss: 1.186720]\n",
      "[Epoch 1/50] [Batch 171/938] [D loss: 1.074422, acc: 90%] [G loss: 1.197057]\n",
      "[Epoch 1/50] [Batch 172/938] [D loss: 1.093974, acc: 92%] [G loss: 1.201827]\n",
      "[Epoch 1/50] [Batch 173/938] [D loss: 1.102966, acc: 89%] [G loss: 1.192162]\n",
      "[Epoch 1/50] [Batch 174/938] [D loss: 1.097220, acc: 91%] [G loss: 1.192569]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-592b258c8c42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0md_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0moptimizer_D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         print(\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/py37/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=200, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
    "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
    "parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\n",
    "parser.add_argument(\"--n_classes\", type=int, default=10, help=\"number of classes for dataset\")\n",
    "parser.add_argument(\"--img_size\", type=int, default=32, help=\"size of each image dimension\")\n",
    "parser.add_argument(\"--channels\", type=int, default=1, help=\"number of image channels\")\n",
    "parser.add_argument(\"--sample_interval\", type=int, default=400, help=\"interval between image sampling\")\n",
    "\n",
    "config_list = ['--n_epochs', '50', '--batch_size', '64']\n",
    "opt = parser.parse_args(config_list)\n",
    "print(opt)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Embedding(opt.n_classes, opt.latent_dim)\n",
    "\n",
    "        self.init_size = opt.img_size // 4  # Initial size before upsampling\n",
    "        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        gen_input = torch.mul(self.label_emb(labels), noise)\n",
    "        out = self.l1(gen_input)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            \"\"\"Returns layers of each discriminator block\"\"\"\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            *discriminator_block(opt.channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = opt.img_size // 2 ** 4\n",
    "\n",
    "        # Output layers\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
    "        self.aux_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, opt.n_classes), nn.Softmax())\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.conv_blocks(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "        label = self.aux_layer(out)\n",
    "\n",
    "        return validity, label\n",
    "\n",
    "\n",
    "# Loss functions\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "auxiliary_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "    auxiliary_loss.cuda()\n",
    "\n",
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Configure data loader\n",
    "os.makedirs(\"../data\", exist_ok=True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"../data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "\n",
    "\n",
    "def sample_image(n_row, batches_done):\n",
    "    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n",
    "    # Sample noise\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, opt.latent_dim))))\n",
    "    # Get labels ranging from 0 to n_classes for n rows\n",
    "    labels = np.array([num for _ in range(n_row) for num in range(n_row)])\n",
    "    labels = Variable(LongTensor(labels))\n",
    "    gen_imgs = generator(z, labels)\n",
    "    save_image(gen_imgs.data, \"images/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
    "\n",
    "\n",
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "for epoch in range(opt.n_epochs):\n",
    "    for i, (imgs, labels) in enumerate(dataloader):\n",
    "\n",
    "        batch_size = imgs.shape[0]\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(FloatTensor))\n",
    "        labels = Variable(labels.type(LongTensor))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise and labels as generator input\n",
    "        z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim))))\n",
    "        gen_labels = Variable(LongTensor(np.random.randint(0, opt.n_classes, batch_size)))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z, gen_labels)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        validity, pred_label = discriminator(gen_imgs)\n",
    "        g_loss = 0.5 * (adversarial_loss(validity, valid) + auxiliary_loss(pred_label, gen_labels))\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Loss for real images\n",
    "        real_pred, real_aux = discriminator(real_imgs)\n",
    "        d_real_loss = (adversarial_loss(real_pred, valid) + auxiliary_loss(real_aux, labels)) / 2\n",
    "\n",
    "        # Loss for fake images\n",
    "        fake_pred, fake_aux = discriminator(gen_imgs.detach())\n",
    "        d_fake_loss = (adversarial_loss(fake_pred, fake) + auxiliary_loss(fake_aux, gen_labels)) / 2\n",
    "\n",
    "        # Total discriminator loss\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "        # Calculate discriminator accuracy\n",
    "        pred = np.concatenate([real_aux.data.cpu().numpy(), fake_aux.data.cpu().numpy()], axis=0)\n",
    "        gt = np.concatenate([labels.data.cpu().numpy(), gen_labels.data.cpu().numpy()], axis=0)\n",
    "        d_acc = np.mean(np.argmax(pred, axis=1) == gt)\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %d%%] [G loss: %f]\"\n",
    "            % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), 100 * d_acc, g_loss.item())\n",
    "        )\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            sample_image(n_row=10, batches_done=batches_done)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
