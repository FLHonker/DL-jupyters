{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Inception_Module](https://img-blog.csdn.net/20160225155336279)\n",
    "![GoogLeNet.png](https://img-blog.csdn.net/20160225155414702)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T08:02:32.326868Z",
     "start_time": "2019-04-23T08:02:32.318091Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Inception\n",
    "class Inception(nn.Module):\n",
    "    def __init__(self, in_planes, kernel_1_x, kernel_3_in, kernel_3_x, kernel_5_in, kernel_5_x, pool_planes):\n",
    "        super(Inception, self).__init__()\n",
    "        # 1x1 conv branch\n",
    "        self.b1 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, kernel_1_x, kernel_size=1),\n",
    "            nn.BatchNorm2d(kernel_1_x),\n",
    "            nn.ReLU(inplace=True),  # can save memery,but override the old variants\n",
    "        )\n",
    "        \n",
    "        # 1x1 conv -> 3x3 conv branch\n",
    "        self.b2 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, kernel_3_in, kernel_size=1),\n",
    "            nn.BatchNorm2d(kernel_3_in),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(kernel_3_in, kernel_3_x, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(kernel_3_x),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        # 1x1 conv -> 5x5 conv branch\n",
    "        self.b3 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, kernel_5_in, kernel_size=1),\n",
    "            nn.BatchNorm2d(kernel_5_in),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(kernel_5_in, kernel_5_x, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(kernel_5_x),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(kernel_5_x, kernel_5_x, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(kernel_5_x),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        # 3x3 pool -> 1x1 conv branch\n",
    "        self.b4 = nn.Sequential(\n",
    "            nn.MaxPool2d(3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_planes, pool_planes, kernel_size=1),\n",
    "            nn.BatchNorm2d(pool_planes),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y1 = self.b1(x)\n",
    "        y2 = self.b2(x)\n",
    "        y3 = self.b3(x)\n",
    "        y4 = self.b4(x)\n",
    "        return torch.cat([y1,y2,y3,y4], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T08:34:07.791746Z",
     "start_time": "2019-04-23T08:34:07.784105Z"
    }
   },
   "outputs": [],
   "source": [
    "# GoogLeNet\n",
    "class GoogLeNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(GoogLeNet, self).__init__()\n",
    "        self.pre_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 192, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        self.a3 = Inception(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)\n",
    "        \n",
    "        self.max_pool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "        \n",
    "        self.a4 = Inception(480, 192,  96, 208, 16,  48,  64)\n",
    "        self.b4 = Inception(512, 160, 112, 224, 24,  64,  64)\n",
    "        self.c4 = Inception(512, 128, 128, 256, 24,  64,  64)\n",
    "        self.d4 = Inception(512, 112, 144, 288, 32,  64,  64)\n",
    "        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)\n",
    "\n",
    "        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
    "        self.linear = nn.Linear(1024, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pre_layers(x)\n",
    "        x = self.a3(x)\n",
    "        x = self.b3(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.a4(x)\n",
    "        x = self.b4(x)\n",
    "        x = self.c4(x)\n",
    "        x = self.d4(x)\n",
    "        x = self.e4(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.a5(x)\n",
    "        x = self.b5(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-23T08:34:09.698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "\n",
      "===> epoch: 1/50\n",
      "Training:\n",
      "\t0/391: loss = 2.3144, Acc = 15.625%\n",
      "\t99/391: loss = 1.5603, Acc = 41.969%\n",
      "\t198/391: loss = 1.3859, Acc = 48.869%\n",
      "\t297/391: loss = 1.2669, Acc = 53.576%\n",
      "(462.0502653121948, 0.56814)\n",
      "Testing:\n",
      "\t0/79: loss = 0.8653, Acc = 64.844%\n",
      "\n",
      "===> epoch: 2/50\n",
      "Training:\n",
      "\t0/391: loss = 0.8877, Acc = 67.188%\n",
      "\t99/391: loss = 0.8085, Acc = 71.094%\n",
      "\t198/391: loss = 0.7766, Acc = 72.444%\n",
      "\t297/391: loss = 0.7540, Acc = 73.382%\n",
      "(283.73740047216415, 0.74518)\n",
      "Testing:\n",
      "\t0/79: loss = 0.7382, Acc = 74.219%\n",
      "\n",
      "===> epoch: 3/50\n",
      "Training:\n",
      "\t0/391: loss = 0.4768, Acc = 82.031%\n",
      "\t99/391: loss = 0.5574, Acc = 80.273%\n",
      "\t198/391: loss = 0.5553, Acc = 80.555%\n",
      "\t297/391: loss = 0.5556, Acc = 80.623%\n",
      "(214.47957506775856, 0.80916)\n",
      "Testing:\n",
      "\t0/79: loss = 0.6552, Acc = 73.438%\n",
      "\n",
      "===> epoch: 4/50\n",
      "Training:\n",
      "\t0/391: loss = 0.4099, Acc = 87.500%\n",
      "\t99/391: loss = 0.4646, Acc = 83.922%\n",
      "\t198/391: loss = 0.4576, Acc = 84.159%\n",
      "\t297/391: loss = 0.4612, Acc = 84.152%\n",
      "(177.15432539582253, 0.84458)\n",
      "Testing:\n",
      "\t0/79: loss = 0.4637, Acc = 79.688%\n",
      "\n",
      "===> epoch: 5/50\n",
      "Training:\n",
      "\t0/391: loss = 0.3862, Acc = 87.500%\n",
      "\t99/391: loss = 0.3782, Acc = 87.031%\n",
      "\t198/391: loss = 0.3810, Acc = 86.950%\n",
      "\t297/391: loss = 0.3832, Acc = 86.986%\n",
      "(150.13100393116474, 0.86912)\n",
      "Testing:\n",
      "\t0/79: loss = 0.3875, Acc = 85.938%\n",
      "\n",
      "===> epoch: 6/50\n",
      "Training:\n",
      "\t0/391: loss = 0.2251, Acc = 94.531%\n",
      "\t99/391: loss = 0.3316, Acc = 88.508%\n",
      "\t198/391: loss = 0.3337, Acc = 88.289%\n",
      "\t297/391: loss = 0.3311, Acc = 88.522%\n",
      "(130.1828045397997, 0.88512)\n",
      "Testing:\n",
      "\t0/79: loss = 0.4271, Acc = 86.719%\n",
      "\n",
      "===> epoch: 7/50\n",
      "Training:\n",
      "\t0/391: loss = 0.1654, Acc = 92.188%\n"
     ]
    }
   ],
   "source": [
    "# %load train_CIFAR-10.py\n",
    "# CIFAR-10 通用训练python脚本\n",
    "# --------------------------------------\n",
    "import torch \n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import argparse\n",
    "\n",
    "# start \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "# main\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='cifar-10 with PyTorch')\n",
    "    parser.add_argument('--lr', default=0.001, type=float, help='learning rate')\n",
    "    parser.add_argument('--epoch', default=50, type=int, help='number of epoch tp train for') \n",
    "    parser.add_argument('--trainBatchSize', default=128, type=int, help='training batch size')\n",
    "    parser.add_argument('--testBatchSize', default=128, type=int, help='testing batch size')\n",
    "    parser.add_argument('--cuda', default=torch.cuda.is_available(), type=bool, help='use cuda or not')\n",
    "    \n",
    "    config_list = ['--lr', '0.001', '--epoch', '50', '--trainBatchSize', '128', '--testBatchSize', '128', '--cuda', 'True']\n",
    "    args = parser.parse_args(config_list) \n",
    "    \n",
    "    solver = Solver(args)\n",
    "    solver.run()\n",
    "\n",
    "CLASSES = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Solver\n",
    "class Solver(object):\n",
    "    def __init__(self, config):\n",
    "        self.model = None\n",
    "        self.lr = config.lr\n",
    "        self.epochs = config.epoch\n",
    "        self.train_batch_size = config.trainBatchSize\n",
    "        self.test_batch_size = config.testBatchSize\n",
    "        self.criterion = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.device = 'cuda' if config.cuda else 'cpu'\n",
    "        self.train_loader = None\n",
    "        self.test_loader = None\n",
    "        \n",
    "    def print_model(self):\n",
    "        print(self.model)\n",
    "        \n",
    "    def load_data(self):\n",
    "        train_transform = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.ToTensor()])\n",
    "        test_transform = transforms.Compose([transforms.ToTensor()])\n",
    "        train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "        test_set = datasets.CIFAR10(root='./data', train=False, download=False, transform=test_transform)\n",
    "        self.train_loader = DataLoader(train_set, batch_size=self.train_batch_size, shuffle=True)\n",
    "        self.test_loader = DataLoader(test_set, batch_size=self.test_batch_size, shuffle=False)\n",
    "    \n",
    "    def load_model(self):\n",
    "        # self.model = LeNet().to(self.device)\n",
    "        # self.model = AlexNet().to(self.device)\n",
    "        # self.model = VGG11().to(self.device)\n",
    "        # self.model = VGG13().to(self.device)\n",
    "        # self.model = VGG16().to(self.device)\n",
    "        # self.model = VGG19().to(self.device)\n",
    "        self.model = GoogLeNet().to(self.device)\n",
    "        # self.model = resnet18().to(self.device)\n",
    "        # self.model = resnet34().to(self.device)\n",
    "        # self.model = resnet50().to(self.device)\n",
    "        # self.model = resnet101().to(self.device)\n",
    "        # self.model = resnet152().to(self.device)\n",
    "        # self.model = DenseNet121().to(self.device)\n",
    "        # self.model = DenseNet161().to(self.device)\n",
    "        # self.model = DenseNet169().to(self.device)\n",
    "        # self.model = DenseNet201().to(self.device)\n",
    "        # self.model = WideResNet(depth=28, num_classes=10).to(self.device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=[75, 150], gamma=0.5)\n",
    "        self.criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "    \n",
    "    # train\n",
    "    def train(self):\n",
    "        print('Training:')\n",
    "        self.model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0 \n",
    "        total = 0 \n",
    "        \n",
    "        for ibatch, (images, labels) in enumerate(self.train_loader):\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "            outputs = self.model(images)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, pred = torch.max(outputs, 1) # second param \"1\" represents the dimension to be reduced\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            # train_correct incremented by one if predicted right\n",
    "            # train_correct += np.sum(prediction[1].cpu().numpy() == target.cpu().numpy())\n",
    "            train_correct += (pred == labels).sum().item()\n",
    "            if ibatch % 99 == 0:\n",
    "                print('\\t{}/{}: loss = {:.4f}, Acc = {:.3f}%'.format(ibatch, len(self.train_loader), train_loss/(ibatch+1), 100. * train_correct/total))\n",
    "        return train_loss, float(train_correct/total)\n",
    "    \n",
    "    # test\n",
    "    def test(self):\n",
    "        print('Testing:')\n",
    "        self.model.eval()\n",
    "        test_loss = 0.0 \n",
    "        test_correct = 0 \n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for ibatch, (images, labels) in enumerate(self.test_loader):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                _, pred = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                test_correct += (pred == labels).sum().item()\n",
    "                if ibatch % 99 == 0:\n",
    "                    print('\\t{}/{}: loss = {:.4f}, Acc = {:.3f}%'.format(ibatch, len(self.test_loader), test_loss/(ibatch+1), 100. * test_correct/total))\n",
    "        return test_loss, float(test_correct/total)\n",
    "    \n",
    "    def save_model(self):\n",
    "        model_out_path = './model/vgg_cifar10.pth'\n",
    "        torch.save(self.model, model_out_path)\n",
    "        print(\"* Checkpoint saved to {}\".format(model_out_path))\n",
    "        \n",
    "    # run\n",
    "    def run(self):\n",
    "        self.load_data()\n",
    "        self.load_model() \n",
    "        accuracy = 0.\n",
    "        \n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.scheduler.step(epoch)\n",
    "            print(\"\\n===> epoch: {}/{}\".format(epoch, self.epochs))\n",
    "            train_result = self.train()\n",
    "            print(train_result)\n",
    "            test_result = self.test()\n",
    "            accuracy = max(accuracy, test_result[1])\n",
    "        print(\"===> BEST ACC. PERFORMANCE: {:.3f}%\".format(accuracy * 100))\n",
    "        self.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
